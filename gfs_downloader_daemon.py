"""
GFS Weather Data Downloader - DAEMON VERSION
Dzia≈Ça w tle, automatycznie sprawdza co 20 minut czy pojawi≈Çy siƒô nowe dane GFS
i pobiera je gdy sƒÖ dostƒôpne.
"""

import sys
import os
import time
import logging
from datetime import datetime, timedelta
import configparser
from sqlalchemy import create_engine, text
import requests

import threading
import queue
from queue import Empty
import warnings
warnings.filterwarnings('ignore')

# Importujemy funkcje z professional version bezpo≈õrednio
# Dodaj katalog do ≈õcie≈ºki Python, ≈ºeby m√≥c zaimportowaƒá modu≈Ç
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# Ustaw flagƒô przed importem, ≈ºeby g≈Ç√≥wny kod siƒô nie wykona≈Ç
import builtins
builtins.__imported_by_daemon__ = True

# Importuj modu≈Ç - kod w if __name__ == "__main__": siƒô nie wykona
# bo mamy ustawionƒÖ flagƒô __imported_by_daemon__
try:
    import gfs_downloader_professional as gfs_professional
    logger_temp = logging.getLogger(__name__)
    logger_temp.info("Modu≈Ç gfs_downloader_professional zaimportowany pomy≈õlnie")
except Exception as e:
    # Je≈õli logger jeszcze nie istnieje, u≈ºyj print
    print(f"B≈ÅƒÑD importu modu≈Çu gfs_downloader_professional: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# === KONFIGURACJA LOGOWANIA ===
LOG_DIR = "logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

# G≈Ç√≥wny plik log√≥w (codzienny)
LOG_FILE = os.path.join(LOG_DIR, f"gfs_daemon_{datetime.now().strftime('%Y%m%d')}.log")

# Szczeg√≥≈Çowy plik log√≥w z wszystkimi operacjami
DETAILED_LOG_FILE = os.path.join(LOG_DIR, f"gfs_daemon_detailed_{datetime.now().strftime('%Y%m%d')}.log")

# Plik log√≥w z b≈Çƒôdami
ERROR_LOG_FILE = os.path.join(LOG_DIR, f"gfs_daemon_errors_{datetime.now().strftime('%Y%m%d')}.log")

# Wycisz logi DEBUG z bibliotek zewnƒôtrznych (przed konfiguracjƒÖ g≈Ç√≥wnego loggera)
logging.getLogger('cfgrib').setLevel(logging.WARNING)
logging.getLogger('ecmwf').setLevel(logging.WARNING)
logging.getLogger('eccodes').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)

# Konfiguracja g≈Ç√≥wnego loggera (konsola + g≈Ç√≥wny plik)
logging.basicConfig(
    level=logging.INFO,  # Zmieniono na INFO - DEBUG tylko dla naszego kodu
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # INFO dla g≈Ç√≥wnego loggera

# Szczeg√≥≈Çowy logger (wszystkie operacje)
detailed_logger = logging.getLogger('detailed')
detailed_logger.setLevel(logging.DEBUG)
detailed_handler = logging.FileHandler(DETAILED_LOG_FILE, encoding='utf-8')
detailed_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
detailed_logger.addHandler(detailed_handler)
detailed_logger.propagate = False

# Logger b≈Çƒôd√≥w
error_logger = logging.getLogger('errors')
error_logger.setLevel(logging.ERROR)
error_handler = logging.FileHandler(ERROR_LOG_FILE, encoding='utf-8')
error_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
error_logger.addHandler(error_handler)
error_logger.propagate = False

# === KONFIGURACJA ===
CHECK_INTERVAL = 1200  # 20 minut w sekundach
WAIT_BETWEEN_ATTEMPTS = 60  # sekund miƒôdzy pr√≥bami pobierania

def load_config():
    """Wczytuje konfiguracjƒô z config.ini"""
    try:
        config = configparser.ConfigParser()
        config.read("config.ini", encoding='utf-8')
        
        return {
            'mysql_user': config["database"]["user"],
            'mysql_password': config["database"]["password"],
            'mysql_host': config["database"]["host"],
            'mysql_database': config["database"]["database"],
            'lat_min': float(config["region"]["lat_min"]),
            'lat_max': float(config["region"]["lat_max"]),
            'lon_min': float(config["region"]["lon_min"]),
            'lon_max': float(config["region"]["lon_max"]),
            'num_threads': 6
        }
    except Exception as e:
        logger.error(f"B≈ÇƒÖd wczytywania konfiguracji: {e}")
        sys.exit(1)

def check_for_new_run(engine, last_run_in_db=None):
    """
    Sprawdza czy pojawi≈Ç siƒô nowy run GFS (sprawdza f000).
    Zoptymalizowane: najpierw sprawdza bazƒô i pomija ju≈º kompletne runy.
    Zwraca (run_time, RUN_DATE, RUN_HOUR) je≈õli znaleziono nowy run, None w przeciwnym razie.
    """
    now_utc = datetime.utcnow()
    current_run_hour = (now_utc.hour // 6) * 6
    run_time = now_utc.replace(hour=current_run_hour, minute=0, second=0, microsecond=0)
    
    detailed_logger.info(f"=== SPRAWDZANIE NOWYCH RUN GFS ===")
    detailed_logger.info(f"Czas sprawdzenia: {now_utc.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    detailed_logger.info(f"Aktualny run (teoretyczny): {run_time.strftime('%Y-%m-%d %H:00')} UTC")
    
    # Sprawd≈∫ najnowszy run w bazie i kt√≥re runy sƒÖ ju≈º kompletne
    # ZAWSZE sprawdzamy bazƒô, ≈ºeby mieƒá aktualne informacje o kompletnych runach
    complete_runs = set()  # Runy kt√≥re majƒÖ wszystkie 209 prognoz
    try:
        with engine.connect() as conn:
            # Pobierz wszystkie runy z bazy i sprawd≈∫ kt√≥re sƒÖ kompletne
            result = conn.execute(text("""
                SELECT DISTINCT run_time
                FROM gfs_forecast
                ORDER BY run_time DESC
            """))
            all_runs_in_db = []
            for row in result:
                run_time_db = row[0]
                if isinstance(run_time_db, str):
                    try:
                        run_time_db = datetime.strptime(run_time_db, '%Y-%m-%d %H:%M:%S')
                    except:
                        try:
                            run_time_db = datetime.strptime(run_time_db, '%Y-%m-%d %H:%M')
                        except:
                            continue
                all_runs_in_db.append(run_time_db)
            
            if all_runs_in_db:
                # Zaktualizuj last_run_in_db je≈õli jest None lub je≈õli w bazie jest nowszy
                max_run_in_db = max(all_runs_in_db)
                if last_run_in_db is None:
                    last_run_in_db = max_run_in_db
                else:
                    # Sprawd≈∫ czy w bazie jest nowszy run
                    if isinstance(last_run_in_db, datetime):
                        last_run_normalized = last_run_in_db.replace(microsecond=0, second=0)
                    else:
                        last_run_normalized = last_run_in_db
                    max_run_normalized = max_run_in_db.replace(microsecond=0, second=0) if isinstance(max_run_in_db, datetime) else max_run_in_db
                    if max_run_normalized > last_run_normalized:
                        last_run_in_db = max_run_in_db
                
                detailed_logger.info(f"Ostatni run w bazie: {last_run_in_db}")
                
                # Sprawd≈∫ kt√≥re runy sƒÖ kompletne (majƒÖ wszystkie 209 prognoz)
                required_hours = gfs_professional.get_required_forecast_hours()
                for run_time_db in all_runs_in_db:
                    try:
                        existing_hours = gfs_professional.get_existing_forecast_hours(run_time_db, engine)
                        missing_hours = required_hours - existing_hours
                        if len(missing_hours) == 0:
                            run_time_normalized = run_time_db.replace(microsecond=0, second=0) if isinstance(run_time_db, datetime) else run_time_db
                            complete_runs.add(run_time_normalized)
                            detailed_logger.info(f"  ‚úì Run {run_time_db.strftime('%Y-%m-%d %H:00')} UTC - kompletny (209/209 prognoz)")
                    except Exception as e:
                        detailed_logger.debug(f"  B≈ÇƒÖd sprawdzania kompletno≈õci run {run_time_db}: {e}")
            else:
                detailed_logger.info("Brak run√≥w w bazie")
                last_run_in_db = None
    except Exception as e:
        detailed_logger.warning(f"B≈ÇƒÖd sprawdzania bazy: {e}")
        error_logger.error(f"B≈ÇƒÖd sprawdzania ostatniego run w bazie: {e}", exc_info=True)
    
    # Sprawd≈∫ do 6 run√≥w wstecz (36h)
    checked_runs = []
    skipped_complete = 0
    
    for i in range(6):
        check_time = run_time - timedelta(hours=i * 6)
        date_str = check_time.strftime("%Y%m%d")
        hour_str = f"{check_time.hour:02d}"
        check_time_normalized = check_time.replace(microsecond=0, second=0)
        
        # NAJPIERW sprawd≈∫ bazƒô - je≈õli run jest kompletny, pomi≈Ñ sprawdzanie online
        if check_time_normalized in complete_runs:
            skipped_complete += 1
            detailed_logger.info(f"  ‚äò Run {check_time.strftime('%Y-%m-%d %H:00')} UTC - POMINIƒòTY (ju≈º kompletny w bazie)")
            continue
        
        # Je≈õli run jest starszy ni≈º ostatni w bazie i jest kompletny, pomi≈Ñ
        if last_run_in_db is not None:
            if isinstance(last_run_in_db, datetime):
                last_run_normalized = last_run_in_db.replace(microsecond=0, second=0)
            else:
                last_run_normalized = last_run_in_db
            
            if check_time_normalized < last_run_normalized:
                # Sprawd≈∫ czy ten starszy run jest kompletny
                try:
                    existing_hours = gfs_professional.get_existing_forecast_hours(check_time, engine)
                    required_hours = gfs_professional.get_required_forecast_hours()
                    missing_hours = required_hours - existing_hours
                    if len(missing_hours) == 0:
                        skipped_complete += 1
                        detailed_logger.info(f"  ‚äò Run {check_time.strftime('%Y-%m-%d %H:00')} UTC - POMINIƒòTY (starszy i kompletny)")
                        continue
                except:
                    pass  # W przypadku b≈Çƒôdu, sprawd≈∫ online
        
        # Tylko teraz sprawd≈∫ dostƒôpno≈õƒá online (tylko dla run√≥w kt√≥re mogƒÖ byƒá potrzebne)
        detailed_logger.info(f"Sprawdzam run: {check_time.strftime('%Y-%m-%d %H:00')} UTC (f000)")
        
        # Sprawd≈∫ dostƒôpno≈õƒá f000 (pierwsza prognoza) - sprawdza oba serwery
        is_available = gfs_professional.check_gfs_availability(date_str, hour_str, 0)
        checked_runs.append({
            'run_time': check_time,
            'available': is_available
        })
        
        if is_available:
            detailed_logger.info(f"  ‚úì Run {check_time.strftime('%Y-%m-%d %H:00')} UTC - f000 DOSTƒòPNA")
            
            # Je≈õli nie mamy run w bazie, zwr√≥ƒá pierwszy dostƒôpny
            if last_run_in_db is None:
                logger.info(f"Znaleziono pierwszy dostƒôpny run: {check_time.strftime('%Y-%m-%d %H:00')} UTC")
                detailed_logger.info(f"  ‚Üí Wybrano: {check_time.strftime('%Y-%m-%d %H:00')} UTC (pierwszy dostƒôpny)")
                return check_time, date_str, hour_str, last_run_in_db
            
            # Normalizuj daty do por√≥wnania
            if isinstance(last_run_in_db, datetime):
                last_run_normalized = last_run_in_db.replace(microsecond=0, second=0)
            else:
                last_run_normalized = last_run_in_db
            
            # Je≈õli ten run jest nowszy ni≈º w bazie, zwr√≥ƒá go
            if check_time_normalized > last_run_normalized:
                logger.info(f"Znaleziono nowszy run: {check_time.strftime('%Y-%m-%d %H:00')} UTC (poprzedni: {last_run_in_db})")
                detailed_logger.info(f"  ‚Üí Wybrano: {check_time.strftime('%Y-%m-%d %H:00')} UTC (nowszy ni≈º w bazie: {last_run_in_db})")
                return check_time, date_str, hour_str, last_run_in_db
            
            # Je≈õli ten sam run, sprawd≈∫ czy ma wszystkie prognozy
            if check_time_normalized == last_run_normalized:
                try:
                    existing_hours = gfs_professional.get_existing_forecast_hours(check_time, engine)
                    required_hours = gfs_professional.get_required_forecast_hours()
                    missing_hours = required_hours - existing_hours
                    
                    if len(missing_hours) == 0:
                        # Wszystkie prognozy sƒÖ ju≈º pobrane
                        detailed_logger.info(f"  ‚Üí Run {check_time.strftime('%Y-%m-%d %H:00')} UTC - wszystkie 209 prognoz ju≈º pobrane")
                        return None, None, None, last_run_in_db
                    else:
                        # Ten sam run, ale brakuje niekt√≥rych prognoz
                        missing_list = sorted(list(missing_hours))[:10]
                        missing_str = ', '.join([f"f{h:03d}" for h in missing_list])
                        if len(missing_hours) > 10:
                            missing_str += f" ... i {len(missing_hours) - 10} wiƒôcej"
                        logger.info(f"Run {check_time.strftime('%Y-%m-%d %H:00')} UTC - brakuje {len(missing_hours)} prognoz")
                        detailed_logger.info(f"  ‚Üí Wybrano: {check_time.strftime('%Y-%m-%d %H:00')} UTC - brakuje {len(missing_hours)} prognoz: {missing_str}")
                        return check_time, date_str, hour_str, last_run_in_db
                except Exception as e:
                    detailed_logger.warning(f"  ‚Üí B≈ÇƒÖd sprawdzania prognoz dla {check_time.strftime('%Y-%m-%d %H:00')} UTC: {e}")
                    error_logger.error(f"B≈ÇƒÖd sprawdzania prognoz dla run {check_time}: {e}", exc_info=True)
                    return check_time, date_str, hour_str, last_run_in_db
        else:
            detailed_logger.info(f"  ‚úó Run {check_time.strftime('%Y-%m-%d %H:00')} UTC - f000 NIEDOSTƒòPNA")
    
    # Podsumowanie sprawdze≈Ñ
    available_runs = [r for r in checked_runs if r['available']]
    detailed_logger.info(f"Podsumowanie: Sprawdzono {len(checked_runs)} run√≥w online, {skipped_complete} pominiƒôtych (ju≈º kompletne), {len(available_runs)} dostƒôpnych")
    if skipped_complete > 0:
        logger.info(f"Pominiƒôto {skipped_complete} ju≈º kompletnych run(√≥w) - nie sprawdzano online")
    detailed_logger.info(f"Brak nowych run√≥w do pobrania")
    
    return None, None, None, last_run_in_db

def download_forecasts(run_time, RUN_DATE, RUN_HOUR, config, engine):
    """
    Pobiera wszystkie prognozy dla danego run.
    U≈ºywa tej samej logiki co professional version z automatycznym ponawianiem.
    """
    logger.info(f"Rozpoczynam pobieranie prognoz dla run {run_time.strftime('%Y-%m-%d %H:00')} UTC")
    detailed_logger.info(f"Rozpoczynam pobieranie prognoz dla run {run_time.strftime('%Y-%m-%d %H:00')} UTC")
    
    # Przygotowanie katalog√≥w
    csv_backup_dir = os.path.join('temp', 'csv_backup')
    if not os.path.exists(csv_backup_dir):
        os.makedirs(csv_backup_dir)
        logger.info(f"Przygotowanie katalogu dla CSV backup...")
        detailed_logger.info(f"Przygotowanie katalogu dla CSV backup: {csv_backup_dir}")
    
    temp_dir = 'temp'
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
        logger.info(f"Przygotowanie katalogu dla plik√≥w tymczasowych...")
        detailed_logger.info(f"Przygotowanie katalogu dla plik√≥w tymczasowych: {temp_dir}")
    
    try:
        logger.debug("Tworzenie ForecastDownloader...")
        downloader = gfs_professional.ForecastDownloader(RUN_DATE, RUN_HOUR, config['lat_min'], config['lat_max'], 
                                        config['lon_min'], config['lon_max'], engine)
        logger.debug("ForecastDownloader utworzony")
    except Exception as e:
        logger.error(f"B≈ÇƒÖd tworzenia ForecastDownloader: {e}", exc_info=True)
        error_logger.error(f"B≈ÇƒÖd tworzenia ForecastDownloader: {e}", exc_info=True)
        raise
    
    try:
        logger.debug("Pobieranie wymaganych godzin prognoz...")
        required_hours = gfs_professional.get_required_forecast_hours()
        logger.debug(f"Wymagane godziny: {len(required_hours)} prognoz")
    except Exception as e:
        logger.error(f"B≈ÇƒÖd pobierania wymaganych godzin: {e}", exc_info=True)
        error_logger.error(f"B≈ÇƒÖd pobierania wymaganych godzin: {e}", exc_info=True)
        raise
    
    try:
        logger.debug("Generowanie listy prognoz...")
        all_forecasts = gfs_professional.generate_forecast_list(run_time)
        logger.debug(f"Wygenerowano {len(all_forecasts)} prognoz")
    except Exception as e:
        logger.error(f"B≈ÇƒÖd generowania listy prognoz: {e}", exc_info=True)
        error_logger.error(f"B≈ÇƒÖd generowania listy prognoz: {e}", exc_info=True)
        raise
    
    attempt = 1
    total_success = 0
    total_failed = 0
    total_records = 0
    
    while True:
        try:
            # Sprawd≈∫ kt√≥re prognozy jeszcze brakujƒÖ
            existing_hours = gfs_professional.get_existing_forecast_hours(run_time, engine)
            missing_hours = sorted(list(required_hours - existing_hours))
            
            if len(missing_hours) == 0:
                logger.info("‚úì‚úì‚úì Wszystkie 209 prognoz sƒÖ ju≈º pobrane!")
                detailed_logger.info("Sprawdzam kompletnosc prognozy...")
                detailed_logger.info("Wszystkie 209 prognoz sƒÖ ju≈º pobrane!")
                break
            
            if attempt > 1:
                logger.info(f"Pr√≥ba #{attempt} - brakuje jeszcze {len(missing_hours)} prognoz")
                detailed_logger.info(f"Sprawdzam kompletnosc prognozy...")
                detailed_logger.info(f"Nastepny plik: {min(missing_hours) if missing_hours else 'brak'}")
            
            # Sprawd≈∫ kompletno≈õƒá przed rozpoczƒôciem
            if attempt == 1:
                logger.info("Sprawdzam kompletnosc prognozy...")
                detailed_logger.info("Sprawdzam kompletnosc prognozy...")
                if missing_hours:
                    detailed_logger.info(f"Nastepny plik: {min(missing_hours)}")
            
            # Filtruj prognozy do pobrania
            forecasts_to_download = [
                f for f in all_forecasts 
                if f['forecast_hour'] in missing_hours
            ]
            
            if len(forecasts_to_download) == 0:
                break
            
            logger.info(f"Pr√≥ba #{attempt}: Pobieranie {len(forecasts_to_download)} brakujƒÖcych prognoz...")
            
            # Przygotuj kolejki i statystyki
            download_queue = queue.Queue()
            progress_queue = queue.Queue()
            stats = {'success': 0, 'failed': 0, 'total_records': 0}
            currently_processing = set()
            
            # Dodaj prognozy do kolejki
            logger.debug(f"Dodawanie {len(forecasts_to_download)} prognoz do kolejki...")
            for forecast in forecasts_to_download:
                download_queue.put(forecast)
            logger.debug(f"Dodano {len(forecasts_to_download)} prognoz do kolejki")
            
            # Uruchom wƒÖtki
            logger.info(f"Uruchamianie {config['num_threads']} wƒÖtk√≥w...")
            detailed_logger.info(f"Uruchamianie {config['num_threads']} wƒÖtk√≥w do pobierania prognoz")
            threads = []
            try:
                for i in range(config['num_threads']):
                    t = threading.Thread(target=gfs_professional.worker_thread, args=(download_queue, downloader, progress_queue, stats, i+1))
                    t.daemon = True
                    t.start()
                    threads.append(t)
                    logger.info(f"WƒÖtek #{i+1} uruchomiony (ID: {t.ident})")
                    detailed_logger.info(f"WƒÖtek #{i+1} uruchomiony (ID: {t.ident})")
                logger.info(f"Wszystkie {len(threads)} wƒÖtki uruchomione")
                detailed_logger.info(f"Wszystkie {len(threads)} wƒÖtki uruchomione")
            except Exception as e:
                logger.error(f"B≈ÇƒÖd uruchamiania wƒÖtk√≥w: {e}", exc_info=True)
                error_logger.error(f"B≈ÇƒÖd uruchamiania wƒÖtk√≥w: {e}", exc_info=True)
                raise
            
            # Progress bar (bez wy≈õwietlania w daemon mode, tylko logowanie)
            processed = 0
            successful_forecasts = []
            failed_forecasts = []
            
            # Logger dla modu≈Çu professional powinien byƒá ju≈º skonfigurowany wcze≈õniej
            # Ale upewnijmy siƒô, ≈ºe jest skonfigurowany
            if not gfs_professional.module_logger.handlers:
                logger.warning("Logger dla modu≈Çu professional nie jest skonfigurowany - konfigurujƒô teraz")
                gfs_professional.module_logger.handlers = []
                gfs_professional.module_logger.addHandler(logging.FileHandler(LOG_FILE, encoding='utf-8'))
                gfs_professional.module_logger.addHandler(detailed_handler)
                gfs_professional.module_logger.addHandler(logging.StreamHandler(sys.stdout))
                gfs_professional.module_logger.addHandler(error_handler)
                gfs_professional.module_logger.setLevel(logging.INFO)  # Zmieniono na INFO
                gfs_professional.module_logger.propagate = False
            
            logger.debug(f"Rozpoczynam przetwarzanie {len(forecasts_to_download)} prognoz w {config['num_threads']} wƒÖtkach")
            
            while processed < len(forecasts_to_download):
                try:
                    progress = progress_queue.get(timeout=30)
                    
                    if progress.get('type') == 'start':
                        forecast_hour = progress['forecast_hour']
                        thread_id = progress.get('thread_id', '?')
                        currently_processing.add(forecast_hour)
                        detailed_logger.info(f"thr: {thread_id} - Rozpoczƒôto pobieranie f{forecast_hour:03d}")
                    
                    elif progress.get('type') == 'done':
                        forecast_hour = progress['forecast_hour']
                        thread_id = progress.get('thread_id', '?')
                        currently_processing.discard(forecast_hour)
                        processed += 1
                        
                        if progress['success']:
                            successful_forecasts.append(forecast_hour)
                            records = progress.get('total_records', 0)
                            logger.info(f"‚úì Pobrano f{forecast_hour:03d} - {records} rekord√≥w")
                            detailed_logger.info(f"thr: {thread_id} - ‚úì Pobrano f{forecast_hour:03d} - {records} rekord√≥w")
                        else:
                            failed_forecasts.append(forecast_hour)
                            logger.warning(f"‚úó B≈ÇƒÖd pobierania f{forecast_hour:03d}")
                            detailed_logger.warning(f"thr: {thread_id} - ‚úó B≈ÅƒÑD pobierania f{forecast_hour:03d}")
                            error_logger.error(f"B≈ÇƒÖd pobierania f{forecast_hour:03d} dla run {run_time.strftime('%Y-%m-%d %H:00')} UTC (wƒÖtek: {thread_id})")
                    
                except Empty:
                    alive = sum(1 for t in threads if t.is_alive())
                    if alive == 0:
                        logger.warning(f"Wszystkie wƒÖtki zako≈Ñczone, ale przetworzono tylko {processed}/{len(forecasts_to_download)} prognoz")
                        break
                except Exception as e:
                    logger.error(f"B≈ÇƒÖd w pƒôtli przetwarzania progress_queue: {e}", exc_info=True)
                    error_logger.error(f"B≈ÇƒÖd w pƒôtli przetwarzania progress_queue: {e}", exc_info=True)
                    # Kontynuuj, ≈ºeby nie przerwaƒá ca≈Çego procesu
                    continue
            
            # Poczekaj na zako≈Ñczenie wƒÖtk√≥w
            for t in threads:
                t.join(timeout=10)
            
            # Sprawd≈∫ czy wszystkie wƒÖtki zako≈Ñczy≈Çy siƒô prawid≈Çowo
            alive_threads = [t for t in threads if t.is_alive()]
            if alive_threads:
                logger.warning(f"Niekt√≥re wƒÖtki nadal dzia≈ÇajƒÖ: {len(alive_threads)}/{len(threads)}")
            
            # Zako≈Ñcz wƒÖtki (wy≈õlij sygna≈Ç None)
            for _ in range(config['num_threads']):
                try:
                    download_queue.put(None, timeout=1)
                except:
                    pass
            
            # Zaktualizuj statystyki
            total_success += stats['success']
            total_failed += stats['failed']
            total_records += stats['total_records']
            
            logger.info(f"Pr√≥ba #{attempt}: Pobrano {stats['success']}, b≈Çƒôd√≥w: {stats['failed']}")
            
            # Szczeg√≥≈Çowe logowanie wynik√≥w pr√≥by
            detailed_logger.info(f"=== PR√ìBA #{attempt} ZAKO≈ÉCZONA ===")
            detailed_logger.info(f"Pobrano: {stats['success']} prognoz")
            detailed_logger.info(f"B≈Çƒôd√≥w: {stats['failed']} prognoz")
            detailed_logger.info(f"Rekord√≥w w bazie: {stats['total_records']}")
            
            if successful_forecasts:
                success_list = sorted(successful_forecasts)
                success_str = ', '.join([f"f{h:03d}" for h in success_list[:20]])
                if len(success_list) > 20:
                    success_str += f" ... i {len(success_list) - 20} wiƒôcej"
                detailed_logger.info(f"Pomy≈õlnie pobrane: {success_str}")
            
            if failed_forecasts:
                failed_list = sorted(failed_forecasts)
                failed_str = ', '.join([f"f{h:03d}" for h in failed_list[:20]])
                if len(failed_list) > 20:
                    failed_str += f" ... i {len(failed_list) - 20} wiƒôcej"
                detailed_logger.warning(f"B≈Çƒôdy pobierania: {failed_str}")
                error_logger.error(f"Pr√≥ba #{attempt}: Nie uda≈Ço siƒô pobraƒá {len(failed_forecasts)} prognoz: {failed_str}")
            
            # Sprawd≈∫ czy wszystkie sƒÖ ju≈º pobrane
            existing_hours_after = gfs_professional.get_existing_forecast_hours(run_time, engine)
            missing_hours_after = sorted(list(required_hours - existing_hours_after))
            
            if len(missing_hours_after) == 0:
                logger.info("‚úì‚úì‚úì Wszystkie 209 prognoz sƒÖ ju≈º pobrane!")
                break
            
            # Je≈õli nie ma nowych sukces√≥w, poczekaj
            if stats['success'] == 0:
                min_missing = min(missing_hours_after) if missing_hours_after else None
                is_available = False
                
                if min_missing is not None:
                    check_hours = sorted(missing_hours_after)[:5]
                    for hour in check_hours:
                        if gfs_professional.check_gfs_availability(RUN_DATE, RUN_HOUR, hour):
                            is_available = True
                            logger.info(f"‚úì Prognoza f{hour:03d} jest dostƒôpna online")
                            break
                
                if not is_available:
                    if min_missing is not None:
                        logger.info(f"‚è≥ Najni≈ºsza brakujƒÖca prognoza: f{min_missing:03d} - jeszcze niedostƒôpna")
                    logger.info(f"‚è≥ Czekam {WAIT_BETWEEN_ATTEMPTS}s przed nastƒôpnƒÖ pr√≥bƒÖ...")
                    time.sleep(WAIT_BETWEEN_ATTEMPTS)
                else:
                    logger.info(f"‚è≥ Czekam {WAIT_BETWEEN_ATTEMPTS}s przed nastƒôpnƒÖ pr√≥bƒÖ...")
                    time.sleep(WAIT_BETWEEN_ATTEMPTS)
            else:
                # By≈Çy sukcesy - kontynuuj szybciej
                logger.info(f"‚úì Pobrano {stats['success']} prognoz. Kontynuujƒô...")
                time.sleep(2)
            
            attempt += 1
            
        except KeyboardInterrupt:
            logger.warning("Przerwano przez u≈ºytkownika (Ctrl+C)")
            logger.info(f"Pobrano ≈ÇƒÖcznie: {total_success} prognoz w {attempt-1} pr√≥bach")
            break
        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas pobierania: {e}", exc_info=True)
            logger.info(f"Czekam {WAIT_BETWEEN_ATTEMPTS}s przed nastƒôpnƒÖ pr√≥bƒÖ...")
            time.sleep(WAIT_BETWEEN_ATTEMPTS)
            attempt += 1
    
    logger.info(f"Pobieranie zako≈Ñczone: {total_success} sukces√≥w, {total_failed} b≈Çƒôd√≥w, {total_records} rekord√≥w")
    
    # Podsumowanie ca≈Çego pobierania
    detailed_logger.info("=" * 70)
    detailed_logger.info(f"=== POBRANIE ZAKO≈ÉCZONE DLA RUN {run_time.strftime('%Y-%m-%d %H:00')} UTC ===")
    detailed_logger.info(f"≈ÅƒÖcznie pr√≥b: {attempt-1}")
    detailed_logger.info(f"Pobrano: {total_success} prognoz")
    detailed_logger.info(f"B≈Çƒôd√≥w: {total_failed} prognoz")
    detailed_logger.info(f"Rekord√≥w w bazie: {total_records}")
    
    # Sprawd≈∫ ko≈Ñcowy stan
    try:
        existing_hours_final = gfs_professional.get_existing_forecast_hours(run_time, engine)
        required_hours = gfs_professional.get_required_forecast_hours()
        missing_hours_final = sorted(list(required_hours - existing_hours_final))
        
        if len(missing_hours_final) == 0:
            detailed_logger.info("‚úì Wszystkie 209 prognoz sƒÖ w bazie!")
        else:
            missing_str = ', '.join([f"f{h:03d}" for h in missing_hours_final[:20]])
            if len(missing_hours_final) > 20:
                missing_str += f" ... i {len(missing_hours_final) - 20} wiƒôcej"
            detailed_logger.warning(f"‚ö† Brakuje jeszcze {len(missing_hours_final)} prognoz: {missing_str}")
    except Exception as e:
        detailed_logger.error(f"B≈ÇƒÖd sprawdzania ko≈Ñcowego stanu: {e}")
        error_logger.error(f"B≈ÇƒÖd sprawdzania ko≈Ñcowego stanu dla run {run_time}: {e}", exc_info=True)
    
    detailed_logger.info("=" * 70)
    
    # === CZYSZCZENIE STARYCH RUN√ìW (zostaw tylko 2 najnowsze kompletne) ===
    if total_success > 0:  # Tylko je≈õli uda≈Ço siƒô pobraƒá co≈õ
        logger.info("Czyszczenie starych run√≥w (zostaw tylko 2 najnowsze kompletne)...")
        detailed_logger.info("Czyszczenie starych run√≥w (zostaw tylko 2 najnowsze kompletne)...")
        
        try:
            with engine.connect() as conn:
                # Znajd≈∫ wszystkie kompletne runy w bazie (majƒÖce wszystkie 209 prognoz)
                required_hours = gfs_professional.get_required_forecast_hours()
                
                # Pobierz wszystkie runy z bazy
                result = conn.execute(text("""
                    SELECT DISTINCT run_time
                    FROM gfs_forecast
                    ORDER BY run_time DESC
                """))
                
                all_runs = []
                complete_runs = []
                
                for row in result:
                    run_time_db = row[0]
                    if isinstance(run_time_db, str):
                        try:
                            run_time_db = datetime.strptime(run_time_db, '%Y-%m-%d %H:%M:%S')
                        except:
                            try:
                                run_time_db = datetime.strptime(run_time_db, '%Y-%m-%d %H:%M')
                            except:
                                continue
                    all_runs.append(run_time_db)
                    
                    # Sprawd≈∫ czy run jest kompletny
                    try:
                        existing_hours = gfs_professional.get_existing_forecast_hours(run_time_db, engine)
                        missing_hours = required_hours - existing_hours
                        if len(missing_hours) == 0:
                            complete_runs.append(run_time_db)
                    except:
                        pass
                
                # Sortuj kompletne runy od najnowszego
                complete_runs.sort(reverse=True)
                
                if len(complete_runs) > 2:
                    # Zachowaj tylko 2 najnowsze kompletne runy
                    runs_to_keep = complete_runs[:2]
                    oldest_kept = min(runs_to_keep)  # Najstarszy z 2 najnowszych kompletnych
                    
                    # Usu≈Ñ tylko runy starsze ni≈º najstarszy z 2 najnowszych kompletnych
                    # (nie usuwamy niekompletnych run√≥w kt√≥re sƒÖ nowsze)
                    runs_to_delete = [rt for rt in all_runs if rt < oldest_kept]
                    
                    if runs_to_delete:
                        deleted_total = 0
                        for old_run in runs_to_delete:
                            try:
                                delete_result = conn.execute(text("""
                                    DELETE FROM gfs_forecast
                                    WHERE run_time = :old_run
                                """), {"old_run": old_run})
                                deleted_count = delete_result.rowcount
                                deleted_total += deleted_count
                                
                                logger.info(f"  ‚úì Usuniƒôto run {old_run.strftime('%Y-%m-%d %H:00')} UTC: {deleted_count} rekord√≥w")
                                detailed_logger.info(f"  ‚úì Usuniƒôto run {old_run.strftime('%Y-%m-%d %H:00')} UTC: {deleted_count} rekord√≥w")
                            except Exception as e:
                                logger.warning(f"  ‚úó B≈ÇƒÖd usuwania run {old_run.strftime('%Y-%m-%d %H:00')} UTC: {e}")
                                detailed_logger.warning(f"  ‚úó B≈ÇƒÖd usuwania run {old_run.strftime('%Y-%m-%d %H:00')} UTC: {e}")
                        
                        conn.commit()
                        
                        logger.info(f"‚úì Usuniƒôto {len(runs_to_delete)} starych run(√≥w) - {deleted_total} rekord√≥w")
                        logger.info(f"  Zosta≈Çy tylko 2 najnowsze kompletne runy:")
                        for rt in runs_to_keep:
                            logger.info(f"    - {rt.strftime('%Y-%m-%d %H:00')} UTC")
                        detailed_logger.info(f"‚úì Usuniƒôto {len(runs_to_delete)} starych run(√≥w) - {deleted_total} rekord√≥w")
                        detailed_logger.info(f"  Zosta≈Çy tylko 2 najnowsze kompletne runy: {[rt.strftime('%Y-%m-%d %H:00') for rt in runs_to_keep]}")
                else:
                    if len(complete_runs) > 0:
                        logger.info(f"‚úì W bazie jest {len(complete_runs)} kompletny(ych) run(√≥w) - wszystko OK")
                        detailed_logger.info(f"‚úì W bazie jest {len(complete_runs)} kompletny(ych) run(√≥w) - wszystko OK")
                    else:
                        logger.info("‚úì Brak kompletnych run√≥w do czyszczenia")
                        detailed_logger.info("‚úì Brak kompletnych run√≥w do czyszczenia")
                        
        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas czyszczenia starych run√≥w: {e}", exc_info=True)
            detailed_logger.error(f"B≈ÇƒÖd podczas czyszczenia starych run√≥w: {e}", exc_info=True)
            error_logger.error(f"B≈ÇƒÖd podczas czyszczenia starych run√≥w: {e}", exc_info=True)
    
    return total_success, total_failed, total_records

def main_daemon_loop():
    """G≈Ç√≥wna pƒôtla daemona"""
    logger.info("=" * 70)
    logger.info("GFS Weather Data Downloader - DAEMON VERSION")
    logger.info("=" * 70)
    logger.info(f"Interwa≈Ç sprawdzania: {CHECK_INTERVAL/60:.0f} minut")
    logger.info(f"Logi zapisywane do:")
    logger.info(f"  - G≈Ç√≥wny log: {LOG_FILE}")
    logger.info(f"  - Szczeg√≥≈Çowy log: {DETAILED_LOG_FILE}")
    logger.info(f"  - Log b≈Çƒôd√≥w: {ERROR_LOG_FILE}")
    logger.info("=" * 70)
    
    detailed_logger.info("=" * 70)
    detailed_logger.info("GFS Weather Data Downloader - DAEMON VERSION - SZCZEG√ì≈ÅOWY LOG")
    detailed_logger.info("=" * 70)
    detailed_logger.info(f"Uruchomiono: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    detailed_logger.info(f"Interwa≈Ç sprawdzania: {CHECK_INTERVAL/60:.0f} minut")
    detailed_logger.info("=" * 70)
    
    # Wczytaj konfiguracjƒô
    config = load_config()
    logger.info(f"Konfiguracja OK - Region: {config['lat_min']}¬∞-{config['lat_max']}¬∞N, {config['lon_min']}¬∞-{config['lon_max']}¬∞E")
    
    # Po≈ÇƒÖcz z bazƒÖ
    try:
        mysql_url = f"mysql+pymysql://{config['mysql_user']}:{config['mysql_password']}@{config['mysql_host']}/{config['mysql_database']}?charset=utf8mb4"
        engine = create_engine(mysql_url, echo=False, pool_pre_ping=True)
        
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        logger.info(f"MySQL OK: {config['mysql_database']}")
    except Exception as e:
        logger.error(f"B≈ÇƒÖd po≈ÇƒÖczenia z MySQL: {e}")
        sys.exit(1)
    
    last_run_in_db = None
    last_check_time = None
    
    logger.info("\nüöÄ Daemon uruchomiony. Dzia≈Ça w tle...")
    logger.info("   (Naci≈õnij Ctrl+C aby zatrzymaƒá)\n")
    
    try:
        while True:
            try:
                current_time = datetime.utcnow()
                
                # Sprawd≈∫ czy minƒÖ≈Ç interwa≈Ç
                if last_check_time is None or (current_time - last_check_time).total_seconds() >= CHECK_INTERVAL:
                    logger.info(f"Sprawdzam dostƒôpno≈õƒá nowych danych GFS... ({current_time.strftime('%Y-%m-%d %H:%M:%S')} UTC)")
                    detailed_logger.info(f"\n{'='*70}")
                    detailed_logger.info(f"SPRAWDZANIE NOWYCH DANYCH - {current_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
                    detailed_logger.info(f"{'='*70}")
                    
                    run_time, RUN_DATE, RUN_HOUR, last_run_in_db = check_for_new_run(engine, last_run_in_db)
                    
                    if run_time is not None:
                        logger.info(f"‚úì Znaleziono run do pobrania: {run_time.strftime('%Y-%m-%d %H:00')} UTC")
                        detailed_logger.info(f"\n{'='*70}")
                        detailed_logger.info(f"ROZPOCZYNAM POBRANIE RUN {run_time.strftime('%Y-%m-%d %H:00')} UTC")
                        detailed_logger.info(f"{'='*70}")
                        
                        # Skonfiguruj logger dla modu≈Çu professional PRZED u≈ºyciem
                        try:
                            gfs_professional.module_logger.handlers = []
                            # Dodaj handler do g≈Ç√≥wnego pliku log√≥w
                            gfs_professional.module_logger.addHandler(logging.FileHandler(LOG_FILE, encoding='utf-8'))
                            # Dodaj handler do szczeg√≥≈Çowego pliku log√≥w
                            gfs_professional.module_logger.addHandler(detailed_handler)
                            # Dodaj handler do konsoli (stdout)
                            gfs_professional.module_logger.addHandler(logging.StreamHandler(sys.stdout))
                            # Dodaj handler do b≈Çƒôd√≥w
                            gfs_professional.module_logger.addHandler(error_handler)
                            gfs_professional.module_logger.setLevel(logging.INFO)  # Zmieniono na INFO ≈ºeby widzieƒá logi
                            gfs_professional.module_logger.propagate = False
                            logger.debug("Logger dla modu≈Çu professional skonfigurowany")
                        except Exception as e:
                            logger.warning(f"B≈ÇƒÖd konfiguracji loggera dla modu≈Çu professional: {e}")
                        
                        # Pobierz wszystkie prognozy
                        try:
                            success, failed, records = download_forecasts(run_time, RUN_DATE, RUN_HOUR, config, engine)
                            
                            # Zaktualizuj last_run_in_db
                            last_run_in_db = run_time
                            
                            logger.info(f"‚úì‚úì‚úì Pobieranie zako≈Ñczone: {success} sukces√≥w, {failed} b≈Çƒôd√≥w")
                        except Exception as e:
                            logger.error(f"KRYTYCZNY B≈ÅƒÑD podczas pobierania prognoz: {e}", exc_info=True)
                            error_logger.error(f"KRYTYCZNY B≈ÅƒÑD podczas pobierania prognoz dla run {run_time}: {e}", exc_info=True)
                            detailed_logger.error(f"KRYTYCZNY B≈ÅƒÑD podczas pobierania prognoz: {e}", exc_info=True)
                            # Kontynuuj dzia≈Çanie daemona zamiast siƒô wy≈ÇƒÖczaƒá
                            logger.info("Kontynuujƒô dzia≈Çanie daemona...")
                    else:
                        logger.info("Brak nowych danych do pobrania")
                        detailed_logger.info("Brak nowych danych do pobrania")
                    
                    last_check_time = current_time
                    next_check = current_time + timedelta(seconds=CHECK_INTERVAL)
                    logger.info(f"Nastƒôpne sprawdzenie za {CHECK_INTERVAL/60:.0f} minut ({next_check.strftime('%Y-%m-%d %H:%M:%S')} UTC)...\n")
                    detailed_logger.info(f"Nastƒôpne sprawdzenie: {next_check.strftime('%Y-%m-%d %H:%M:%S')} UTC\n")
                
                # Czekaj 1 minutƒô przed nastƒôpnym sprawdzeniem
                time.sleep(60)
                
            except KeyboardInterrupt:
                logger.info("\n‚ö†Ô∏è  Zatrzymywanie daemona...")
                break
            except Exception as e:
                logger.error(f"B≈ÇƒÖd w g≈Ç√≥wnej pƒôtli: {e}", exc_info=True)
                time.sleep(60)  # Czekaj przed ponownƒÖ pr√≥bƒÖ
                
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Daemon zatrzymany przez u≈ºytkownika")
    except SystemExit as e:
        logger.error(f"SystemExit wywo≈Çany: {e}", exc_info=True)
        error_logger.error(f"SystemExit wywo≈Çany: {e}", exc_info=True)
        raise  # Pozw√≥l na normalne wyj≈õcie
    except Exception as e:
        logger.error(f"Krytyczny b≈ÇƒÖd w g≈Ç√≥wnej pƒôtli daemona: {e}", exc_info=True)
        error_logger.error(f"Krytyczny b≈ÇƒÖd w g≈Ç√≥wnej pƒôtli daemona: {e}", exc_info=True)
        detailed_logger.error(f"Krytyczny b≈ÇƒÖd w g≈Ç√≥wnej pƒôtli daemona: {e}", exc_info=True)
        # Nie ko≈Ñcz programu - spr√≥buj kontynuowaƒá
        logger.info("Pr√≥bujƒô kontynuowaƒá dzia≈Çanie daemona...")
        time.sleep(60)  # Poczekaj przed ponownƒÖ pr√≥bƒÖ
    finally:
        logger.info("Daemon zako≈Ñczony")
        detailed_logger.info("Daemon zako≈Ñczony")

if __name__ == "__main__":
    main_daemon_loop()


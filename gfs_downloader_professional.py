"""
GFS Weather Data Downloader - PROFESSIONAL VERSION
Pobiera pe≈Çny zakres prognoz: f000-f120 (co 1h) + f123-f384 (co 3h) = 209 prognoz
Z multi-threading, resume, progress bar i priorytetyzacjƒÖ
"""

import xarray as xr
import pandas as pd
import numpy as np
import requests
import os
import configparser
from datetime import datetime, timedelta
from sqlalchemy import create_engine, text
import threading
import queue
from queue import Empty
import time
from tqdm import tqdm
import warnings
import os
import logging
warnings.filterwarnings('ignore')

# St≈Çum b≈Çƒôdy ECCODES (sƒÖ tylko ostrze≈ºeniami)
os.environ['ECCODES_LOG_VERBOSITY'] = '0'
os.environ['ECCODES_DEBUG'] = '0'

# Wycisz logi DEBUG z cfgrib, ecmwf, eccodes, urllib3, requests (niepotrzebne dla u≈ºytkownika)
logging.getLogger('cfgrib').setLevel(logging.WARNING)
logging.getLogger('ecmwf').setLevel(logging.WARNING)
logging.getLogger('eccodes').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('urllib3.connectionpool').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)

# Logger dla modu≈Çu (bƒôdzie u≈ºywa≈Ç root logger je≈õli nie jest skonfigurowany)
module_logger = logging.getLogger(__name__)

# === G≈Å√ìWNY KOD - WYKONUJE SIƒò TYLKO GDY URUCHOMIONY BEZPO≈öREDNIO ===
# Sprawd≈∫ czy modu≈Ç jest uruchamiany bezpo≈õrednio (nie importowany)
try:
    import builtins
    _is_imported_by_daemon = hasattr(builtins, '__imported_by_daemon__')
except:
    _is_imported_by_daemon = False

_is_main_module = (__name__ == "__main__" and not _is_imported_by_daemon)
if _is_main_module:
    print("=" * 70)
    print("GFS Weather Data Downloader - PROFESSIONAL VERSION")
    print("=" * 70)

    # === 1. KONFIGURACJA ===
    try:
        config = configparser.ConfigParser()
        config.read("config.ini", encoding='utf-8')
        
        MYSQL_USER = config["database"]["user"]
        MYSQL_PASSWORD = config["database"]["password"]
        MYSQL_HOST = config["database"]["host"]
        MYSQL_DATABASE = config["database"]["database"]
        
        lat_min = float(config["region"]["lat_min"])
        lat_max = float(config["region"]["lat_max"])
        lon_min = float(config["region"]["lon_min"])
        lon_max = float(config["region"]["lon_max"])
        
        # Konfiguracja wƒÖtk√≥w (mo≈ºna dostosowaƒá)
        NUM_THREADS = 6  # 4-8 wƒÖtk√≥w r√≥wnolegle
        
        print(f"‚úì Konfiguracja OK")
        print(f"  Region: {lat_min}¬∞-{lat_max}¬∞N, {lon_min}¬∞-{lon_max}¬∞E")
        print(f"  WƒÖtki: {NUM_THREADS}")
        
    except Exception as e:
        print(f"‚úó B≈ÅƒÑD konfiguracji: {e}")
        input("\nEnter...")
        exit(1)

    # === 2. PO≈ÅƒÑCZENIE Z BAZƒÑ ===
    try:
        print(f"\n‚è≥ ≈ÅƒÖczenie z MySQL...")
        
        MYSQL_URL = f"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}/{MYSQL_DATABASE}?charset=utf8mb4"
        engine = create_engine(MYSQL_URL, echo=False, pool_pre_ping=True)
        
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        print(f"‚úì MySQL OK: {MYSQL_DATABASE}")
        
    except Exception as e:
        print(f"‚úó B≈ÅƒÑD MySQL: {e}")
        input("\nEnter...")
        exit(1)

# === 3. FUNKCJE POMOCNICZE ===

def check_gfs_availability(date_str, hour_str, forecast_hour, verbose=False):
    """
    Sprawdza czy dana prognoza GFS jest dostƒôpna.
    Sprawdza oba serwery: nomads.ncep.noaa.gov i ftp.ncep.noaa.gov
    Zwraca True je≈õli kt√≥rykolwiek serwer ma dane dostƒôpne.
    """
    # Lista serwer√≥w do sprawdzenia (w kolejno≈õci priorytetu)
    servers = [
        "nomads.ncep.noaa.gov",
        "ftp.ncep.noaa.gov"
    ]
    
    base_path = f"/pub/data/nccf/com/gfs/prod/gfs.{date_str}/{hour_str}/atmos/gfs.t{hour_str}z.pgrb2.0p25.f{forecast_hour:03d}"
    
    for server in servers:
        url = f"https://{server}{base_path}"
        
        try:
            # U≈ºywamy HEAD zamiast GET dla szybszego sprawdzenia
            response = requests.head(url, timeout=10, allow_redirects=True)
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                # Sprawd≈∫ czy to nie jest strona HTML (b≈ÇƒÖd 404 jako HTML)
                if 'text/html' in content_type:
                    continue  # Spr√≥buj nastƒôpny serwer
                
                if verbose:
                    module_logger.debug(f"  ‚úì Dane dostƒôpne na {server}")
                return True
                
        except requests.exceptions.Timeout:
            if verbose:
                module_logger.debug(f"  ‚è± Timeout na {server}")
            continue  # Spr√≥buj nastƒôpny serwer
        except requests.exceptions.RequestException as e:
            if verbose:
                module_logger.debug(f"  ‚úó B≈ÇƒÖd na {server}: {e}")
            continue  # Spr√≥buj nastƒôpny serwer
        except Exception as e:
            if verbose:
                module_logger.debug(f"  ‚úó Nieoczekiwany b≈ÇƒÖd na {server}: {e}")
            continue  # Spr√≥buj nastƒôpny serwer
    
    # Je≈õli ≈ºaden serwer nie zwr√≥ci≈Ç 200, sprawd≈∫ jeszcze raz przez GET (dla pewno≈õci)
    # Niekt√≥re serwery mogƒÖ nie obs≈Çugiwaƒá HEAD poprawnie
    for server in servers:
        url = f"https://{server}{base_path}"
        
        try:
            response = requests.get(url, stream=True, timeout=10)
            response.close()
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '').lower()
                if 'text/html' in content_type:
                    continue
                
                if verbose:
                    module_logger.debug(f"  ‚úì Dane dostƒôpne na {server} (GET)")
                return True
                
        except:
            continue
    
    return False

def get_required_forecast_hours():
    """
    Zwraca set wymaganych forecast_hour do pobrania:
    - f000-f120 (co 1h) = 121 prognoz
    - f123-f384 (co 3h) = 88 prognoz
    RAZEM: 209 prognoz
    """
    required_hours = set()
    
    # f000-f120: co 1h (121 prognoz)
    for hour in range(0, 121):
        required_hours.add(hour)
    
    # f123-f384: co 3h (88 prognoz)
    for hour in range(123, 385, 3):
        required_hours.add(hour)
    
    return required_hours

def get_existing_forecast_hours(run_time, engine=None):
    """
    Zwraca set forecast_hour kt√≥re sƒÖ ju≈º w bazie dla danego run_time.
    Oblicza forecast_hour na podstawie r√≥≈ºnicy miƒôdzy forecast_time a run_time.
    """
    # U≈ºyj globalnego engine je≈õli nie przekazano
    if engine is None:
        try:
            engine = globals().get('engine')
            if engine is None:
                return set()
        except:
            return set()
    
    try:
        with engine.connect() as conn:
            run_time_str = run_time.strftime('%Y-%m-%d %H:%M:%S')
            
            result = conn.execute(text("""
                SELECT DISTINCT forecast_time
                FROM gfs_forecast
                WHERE DATE_FORMAT(run_time, '%Y-%m-%d %H:%i:%s') = :run_time
                ORDER BY forecast_time
            """), {"run_time": run_time_str})
            
            existing_hours = set()
            rows = result.fetchall()
            
            for row in rows:
                forecast_time = row[0]
                
                # Parsuj forecast_time je≈õli to string
                if isinstance(forecast_time, str):
                    try:
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M']:
                            try:
                                forecast_time = datetime.strptime(forecast_time, fmt)
                                break
                            except:
                                continue
                    except:
                        continue
                
                if isinstance(forecast_time, datetime):
                    # Oblicz forecast_hour jako r√≥≈ºnicƒô w godzinach
                    time_diff = forecast_time - run_time
                    forecast_hour = int(time_diff.total_seconds() / 3600)
                    existing_hours.add(forecast_hour)
            
            return existing_hours
            
    except Exception as e:
        print(f"‚ö† B≈ÇƒÖd sprawdzania forecast_hour w bazie: {e}")
        return set()

def find_latest_gfs_run(engine=None):
    """Znajduje najnowszy dostƒôpny run GFS (szuka nowszego ni≈º w bazie)"""
    # U≈ºyj globalnego engine je≈õli nie przekazano
    if engine is None:
        try:
            engine = globals().get('engine')
            if engine is None:
                return None, None, None
        except:
            return None, None, None
    
    now_utc = datetime.utcnow()
    current_run_hour = (now_utc.hour // 6) * 6
    run_time = now_utc.replace(hour=current_run_hour, minute=0, second=0, microsecond=0)
    
    # Pobierz wymagane forecast_hour
    required_hours = get_required_forecast_hours()
    
    # Sprawd≈∫ najnowszy run w bazie
    last_run_in_db = None
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT MAX(run_time) as last_run
                FROM gfs_forecast
            """))
            row = result.fetchone()
            if row and row[0]:
                last_run_in_db = row[0]
                if isinstance(last_run_in_db, str):
                    try:
                        last_run_in_db = datetime.strptime(last_run_in_db, '%Y-%m-%d %H:%M:%S')
                    except:
                        try:
                            last_run_in_db = datetime.strptime(last_run_in_db, '%Y-%m-%d %H:%M')
                        except:
                            pass
    except:
        pass
    
    # Szukaj nowszego run ni≈º ten w bazie
    found_run = None
    
    for i in range(6):  # Sprawd≈∫ do 6 run√≥w wstecz (36h)
        check_time = run_time - timedelta(hours=i * 6)
        date_str = check_time.strftime("%Y%m%d")
        hour_str = f"{check_time.hour:02d}"
        
        # Sprawd≈∫ dostƒôpno≈õƒá online
        if check_gfs_availability(date_str, hour_str, 0):
            # Je≈õli nie mamy run w bazie, zwr√≥ƒá pierwszy dostƒôpny
            if last_run_in_db is None:
                return check_time, date_str, hour_str
            
            # Normalizuj daty do por√≥wnania (usu≈Ñ mikrosekundy)
            check_time_normalized = check_time.replace(microsecond=0, second=0)
            if isinstance(last_run_in_db, datetime):
                last_run_normalized = last_run_in_db.replace(microsecond=0, second=0)
            else:
                last_run_normalized = last_run_in_db
            
            # Je≈õli ten run jest starszy ni≈º w bazie, pomi≈Ñ
            if check_time_normalized < last_run_normalized:
                continue
            
            # Je≈õli ten run jest taki sam jak w bazie, sprawd≈∫ czy ma wszystkie wymagane prognozy
            if check_time_normalized == last_run_normalized:
                try:
                    # Sprawd≈∫ kt√≥re konkretne forecast_hour sƒÖ ju≈º w bazie
                    existing_hours = get_existing_forecast_hours(check_time, engine)
                    missing_hours = required_hours - existing_hours
                    
                    # Je≈õli wszystkie wymagane prognozy sƒÖ ju≈º pobrane, szukaj nowszego run
                    if len(missing_hours) == 0:
                        # Szukaj nowszego run
                        continue
                    else:
                        # Ten sam run, ale brakuje niekt√≥rych prognoz - zwr√≥ƒá go
                        found_run = (check_time, date_str, hour_str)
                        break
                except Exception as e:
                    # W przypadku b≈Çƒôdu, za≈Ç√≥≈º ≈ºe brakuje prognoz
                    found_run = (check_time, date_str, hour_str)
                    break
            
            # Ten run jest nowszy ni≈º w bazie - zwr√≥ƒá go
            if check_time_normalized > last_run_normalized:
                return check_time, date_str, hour_str
            
            # Ten sam run, ale mo≈ºe nie mieƒá wszystkich prognoz
            if found_run is None:
                found_run = (check_time, date_str, hour_str)
    
    # Je≈õli nie znaleziono nowszego, zwr√≥ƒá ten sam je≈õli nie ma wszystkich prognoz
    if found_run:
        return found_run
    
    return None, None, None

def check_existing_forecasts(run_time, engine=None):
    """Sprawdza kt√≥re prognozy ju≈º sƒÖ w bazie dla danego run_time"""
    # U≈ºyj globalnego engine je≈õli nie przekazano
    if engine is None:
        try:
            engine = globals().get('engine')
            if engine is None:
                return set()
        except:
            return set()
    
    try:
        with engine.connect() as conn:
            # Formatuj run_time jako string dla por√≥wnania w SQL
            run_time_str = run_time.strftime('%Y-%m-%d %H:%M:%S')
            
            # U≈ºyj formatu datetime dla por√≥wnania
            result = conn.execute(text("""
                SELECT DISTINCT forecast_time
                FROM gfs_forecast
                WHERE DATE_FORMAT(run_time, '%Y-%m-%d %H:%i:%s') = :run_time
                ORDER BY forecast_time
            """), {"run_time": run_time_str})
            
            existing_times = set()
            rows = result.fetchall()
            
            for row in rows:
                # Upewnij siƒô ≈ºe to datetime object
                forecast_time = row[0]
                
                # Normalizuj datƒô - usu≈Ñ mikrosekundy dla por√≥wnania
                if isinstance(forecast_time, datetime):
                    # ZaokrƒÖglij do sekundy (usu≈Ñ mikrosekundy)
                    normalized = forecast_time.replace(microsecond=0, second=0)
                    existing_times.add(normalized)
                elif isinstance(forecast_time, str):
                    # Je≈õli string, parsuj do datetime
                    try:
                        # Spr√≥buj r√≥≈ºne formaty
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M']:
                            try:
                                dt = datetime.strptime(forecast_time, fmt)
                                normalized = dt.replace(microsecond=0, second=0)
                                existing_times.add(normalized)
                                break
                            except:
                                continue
                    except:
                        pass
            
            return existing_times
            
    except Exception as e:
        print(f"‚ö† B≈ÇƒÖd sprawdzania bazy: {e}")
        import traceback
        traceback.print_exc()
        return set()

def generate_forecast_list(run_time):
    """
    Generuje listƒô prognoz do pobrania:
    - f000-f120 (co 1h) = 121 prognoz
    - f123-f384 (co 3h) = 88 prognoz
    RAZEM: 209 prognoz
    Priorytet: naj≈õwie≈ºsze pierwsze
    """
    forecasts = []
    
    # f000-f120: co 1h (121 prognoz)
    for hour in range(0, 121):
        forecast_time = run_time + timedelta(hours=hour)
        forecasts.append({
            'forecast_hour': hour,
            'forecast_time': forecast_time,
            'priority': hour  # Ni≈ºszy = wy≈ºszy priorytet (f000, f001, f002...)
        })
    
    # f123-f384: co 3h (88 prognoz)
    for hour in range(123, 385, 3):
        forecast_time = run_time + timedelta(hours=hour)
        forecasts.append({
            'forecast_hour': hour,
            'forecast_time': forecast_time,
            'priority': hour  # Ni≈ºszy = wy≈ºszy priorytet
        })
    
    # Sortuj wed≈Çug priorytetu (naj≈õwie≈ºsze pierwsze)
    forecasts.sort(key=lambda x: x['priority'])
    
    return forecasts

    # === 4. ZNAJD≈π NAJNOWSZY RUN ===
    print(f"\n‚è≥ Szukam najnowszego run GFS...")

    try:
        run_time, RUN_DATE, RUN_HOUR = find_latest_gfs_run(engine)
        
        if run_time is None:
            # Sprawd≈∫ co mamy w bazie
            try:
                with engine.connect() as conn:
                    result = conn.execute(text("""
                        SELECT MAX(run_time) as last_run, COUNT(DISTINCT forecast_time) as count
                        FROM gfs_forecast
                        WHERE run_time = (SELECT MAX(run_time) FROM gfs_forecast)
                    """))
                row = result.fetchone()
                if row and row[0]:
                    last_run = row[0]
                    count = row[1] if row[1] else 0
                    
                    # Sprawd≈∫ kt√≥re konkretne prognozy sƒÖ ju≈º pobrane
                    if isinstance(last_run, str):
                        try:
                            last_run = datetime.strptime(last_run, '%Y-%m-%d %H:%M:%S')
                        except:
                            try:
                                last_run = datetime.strptime(last_run, '%Y-%m-%d %H:%M')
                            except:
                                pass
                    
                    required_hours = get_required_forecast_hours()
                    existing_hours = get_existing_forecast_hours(last_run, engine) if isinstance(last_run, datetime) else set()
                    missing_hours = sorted(list(required_hours - existing_hours))
                    
                    print("\n" + "=" * 70)
                    print("‚ÑπÔ∏è  BRAK NOWSZEGO RUN GFS")
                    print("=" * 70)
                    print(f"Ostatni run w bazie: {last_run}")
                    print(f"Prognoz w bazie: {count} / 209")
                    
                    if len(missing_hours) == 0:
                        print(f"\nüí° Wszystkie dane sƒÖ aktualne!")
                    else:
                        print(f"\n‚ö†Ô∏è  Brakuje {len(missing_hours)} prognoz:")
                        # Poka≈º pierwsze 20 brakujƒÖcych prognoz
                        missing_str = ', '.join([f"f{h:03d}" for h in missing_hours[:20]])
                        if len(missing_hours) > 20:
                            missing_str += f" ... i {len(missing_hours) - 20} wiƒôcej"
                        print(f"   BrakujƒÖce: {missing_str}")
                        print(f"\nüí° Uruchom ponownie skrypt, aby pobraƒá brakujƒÖce prognozy.")
                    
                    # Oblicz kiedy bƒôdzie nastƒôpny run
                    if isinstance(last_run, datetime):
                        next_run = last_run + timedelta(hours=6)
                        print(f"\nNastƒôpny run GFS: {next_run.strftime('%Y-%m-%d %H:00')} UTC")
                        
                        # Kiedy bƒôdzie dostƒôpny (ok. 3.5h po run)
                        next_available = next_run + timedelta(hours=3, minutes=30)
                        print(f"Bƒôdzie dostƒôpny oko≈Ço: {next_available.strftime('%Y-%m-%d %H:%M')} UTC")
                    print("=" * 70)
            except:
                print("‚úó Nie znaleziono dostƒôpnego run GFS")
            
            input("\nNaci≈õnij Enter...")
            exit(0)
        
        print(f"‚úì Run znaleziony: {run_time.strftime('%Y-%m-%d %H:00')} UTC")
        
    except Exception as e:
        print(f"‚úó B≈ÅƒÑD: {e}")
        input("\nEnter...")
        exit(1)

    # === 5. SPRAWD≈π CO JU≈ª JEST W BAZIE (RESUME) ===
    print(f"\n‚è≥ Sprawdzam co ju≈º jest w bazie...")

    # Debug: sprawd≈∫ wszystkie run_time w bazie
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT DISTINCT run_time, COUNT(DISTINCT forecast_time) as count, COUNT(*) as total_records
                FROM gfs_forecast
                GROUP BY run_time
                ORDER BY run_time DESC
                LIMIT 5
            """))
            all_runs = list(result)
            if all_runs:
                print(f"  Znalezione run_time w bazie:")
                for row in all_runs:
                    rt, count, total = row[0], row[1], row[2]
                    print(f"    - {rt}: {count} unikalnych prognoz, {total} rekord√≥w")
    except Exception as e:
        print(f"  ‚ö† B≈ÇƒÖd sprawdzania run_time: {e}")

    existing_forecast_times = check_existing_forecasts(run_time, engine)
    existing_count = len(existing_forecast_times)

    # Sprawd≈∫ kt√≥re konkretne forecast_hour sƒÖ ju≈º pobrane
    existing_hours = get_existing_forecast_hours(run_time, engine)
    required_hours = get_required_forecast_hours()
    missing_hours = sorted(list(required_hours - existing_hours))

    # Debug: poka≈º przyk≈Çadowe forecast_time z bazy
    if existing_count > 0:
        print(f"‚úì Znaleziono {existing_count} istniejƒÖcych prognoz w bazie dla run {run_time.strftime('%Y-%m-%d %H:00')}")
        if len(missing_hours) > 0:
            print(f"  ‚ö† Brakuje {len(missing_hours)} prognoz (bƒôdƒô je pobieraƒá)")
            # Poka≈º pierwsze 10 brakujƒÖcych prognoz
            missing_str = ', '.join([f"f{h:03d}" for h in missing_hours[:10]])
            if len(missing_hours) > 10:
                missing_str += f" ... i {len(missing_hours) - 10} wiƒôcej"
            print(f"  BrakujƒÖce: {missing_str}")
        else:
            print(f"  ‚úì Wszystkie wymagane prognozy sƒÖ ju≈º w bazie")
        print(f"  Bƒôdƒô kontynuowaƒá od miejsca przerwania (RESUME)")
        # Debug: poka≈º pierwsze 3 i ostatnie 3 przyk≈Çady
        sorted_times = sorted(list(existing_forecast_times))
        if sorted_times:
            first_3 = sorted_times[:3]
            last_3 = sorted_times[-3:] if len(sorted_times) > 3 else []
            examples = ', '.join([t.strftime('%Y-%m-%d %H:%M') for t in first_3])
            if last_3:
                examples += f" ... {', '.join([t.strftime('%Y-%m-%d %H:%M') for t in last_3])}"
            print(f"  Przyk≈Çady: {examples}")
    else:
        print(f"‚úì Brak prognoz dla tego run - zaczynam od poczƒÖtku")

    # === 6. GENERUJ LISTƒò PROGNOZ ===
    print(f"\n‚è≥ Generowanie listy prognoz...")

    all_forecasts = generate_forecast_list(run_time)
    print(f"‚úì Wygenerowano {len(all_forecasts)} prognoz do pobrania")

    # Filtruj te kt√≥re ju≈º sƒÖ w bazie
    # Normalizuj daty prognoz (usu≈Ñ mikrosekundy) dla por√≥wnania
    forecasts_to_download = []
    skipped_forecasts = []

    for f in all_forecasts:
        # Normalizuj forecast_time (usu≈Ñ mikrosekundy i sekundy)
        normalized_forecast_time = f['forecast_time'].replace(microsecond=0, second=0)
        
        # Sprawd≈∫ czy ju≈º jest w bazie
        if normalized_forecast_time not in existing_forecast_times:
            forecasts_to_download.append(f)
        else:
            skipped_forecasts.append(f)

    print(f"‚úì Do pobrania: {len(forecasts_to_download)} prognoz")
    print(f"‚úì Ju≈º w bazie: {existing_count} prognoz")

    # Debug: poka≈º pierwsze i ostatnie pominiƒôte prognozy
    if skipped_forecasts and len(skipped_forecasts) <= 10:
        skipped_str = ', '.join([f"f{f['forecast_hour']:03d}({f['forecast_time'].strftime('%Y-%m-%d %H:%M')})" for f in skipped_forecasts])
        print(f"  Pominiƒôte prognozy: {skipped_str}")
    elif skipped_forecasts:
        first_3 = skipped_forecasts[:3]
        last_3 = skipped_forecasts[-3:]
        first_str = ', '.join([f"f{f['forecast_hour']:03d}({f['forecast_time'].strftime('%H:%M')})" for f in first_3])
        last_str = ', '.join([f"f{f['forecast_hour']:03d}({f['forecast_time'].strftime('%H:%M')})" for f in last_3])
        print(f"  Pominiƒôte (pierwsze): {first_str}")
        print(f"  Pominiƒôte (ostatnie): {last_str}")

    if len(forecasts_to_download) == 0:
        print("\n" + "=" * 70)
        print("‚ÑπÔ∏è  WSZYSTKIE PROGNOZY JU≈ª SƒÑ W BAZIE!")
        print("=" * 70)
        print(f"Run: {run_time.strftime('%Y-%m-%d %H:00')} UTC")
        print(f"Prognoz w bazie: {existing_count} / {len(all_forecasts)}")
        input("\nNaci≈õnij Enter...")
        exit(0)

    # === 7. KLASY I FUNKCJE DO MULTI-THREADING ===

class ForecastDownloader:
    def __init__(self, run_date, run_hour, lat_min, lat_max, lon_min, lon_max, engine):
        self.run_date = run_date
        self.run_hour = run_hour
        self.lat_min = lat_min
        self.lat_max = lat_max
        self.lon_min = lon_min
        self.lon_max = lon_max
        self.engine = engine
        self.filters_config = [
            # Ci≈õnienie
            {'name': 'mslp', 'filter': {'typeOfLevel': 'meanSea', 'stepType': 'instant'}, 'vars': ['prmsl']},
            
            # Opady - u≈ºywamy stepType 'accum' dla skumulowanych opad√≥w od poczƒÖtku prognozy
            # Dla f000 tp=0 (brak opad√≥w), dla f003 tp=opady w ciƒÖgu 3h od poczƒÖtku, itd.
            {'name': 'precip', 'filter': {'typeOfLevel': 'surface', 'stepType': 'accum', 'shortName': 'tp'}, 'vars': ['tp']},
            {'name': 'precip_rate', 'filter': {'typeOfLevel': 'surface', 'stepType': 'instant'}, 'vars': ['prate']},
            
            # Zachmurzenie - wszystkie poziomy
            {'name': 'clouds', 'filter': {'typeOfLevel': 'surface', 'stepType': 'instant'}, 'vars': ['tcc', 'lcc', 'mcc', 'hcc']},
            
            # Parametry 2m
            {'name': 't2m', 'filter': {'typeOfLevel': 'heightAboveGround', 'level': 2, 'stepType': 'instant'}, 'vars': ['t2m', 'd2m', 'r2']},
            
            # Wiatr 10m
            {'name': 'wind10', 'filter': {'typeOfLevel': 'heightAboveGround', 'level': 10, 'stepType': 'instant'}, 'vars': ['u10', 'v10', 'gust']},
            
            # Wiatr 80m
            {'name': 'wind80', 'filter': {'typeOfLevel': 'heightAboveGround', 'level': 80, 'stepType': 'instant'}, 'vars': ['u', 'v', 't']},
            
            # Parametry atmosferyczne
            {'name': 'cape', 'filter': {'typeOfLevel': 'atmosphere', 'stepType': 'instant'}, 'vars': ['cape', 'cin', 'pwat']},
            
            # Parametry wysoko≈õciowe 850 hPa
            {'name': 't850', 'filter': {'typeOfLevel': 'isobaricInhPa', 'level': 850}, 'vars': ['t', 'gh']},
            
            # Parametry wysoko≈õciowe 500 hPa
            {'name': 'gh500', 'filter': {'typeOfLevel': 'isobaricInhPa', 'level': 500}, 'vars': ['gh']},
            
            # Widzialno≈õƒá i promieniowanie
            {'name': 'surface_other', 'filter': {'typeOfLevel': 'surface', 'stepType': 'instant'}, 'vars': ['vis', 'dswrf']},
        ]
    
    def download_and_process(self, forecast_info, progress_queue, thread_id=None, attempt_count=0):
        """
        Pobiera i przetwarza jednƒÖ prognozƒô
        Zwraca (success, forecast_info, df) lub (False, forecast_info, None)
        """
        forecast_hour = forecast_info['forecast_hour']
        forecast_time = forecast_info['forecast_time']
        run_time = datetime.strptime(f"{self.run_date} {self.run_hour}", "%Y%m%d %H")
        
        # Lista serwer√≥w do sprawdzenia (w kolejno≈õci priorytetu)
        servers = [
            "nomads.ncep.noaa.gov",
            "ftp.ncep.noaa.gov"
        ]
        
        base_path = f"/pub/data/nccf/com/gfs/prod/gfs.{self.run_date}/{self.run_hour}/atmos/gfs.t{self.run_hour}z.pgrb2.0p25.f{forecast_hour:03d}"
        idx_path = f"{base_path}.idx"
        
        temp_file = None
        response = None
        used_server = None
        
        # NAJPIERW sprawd≈∫ czy plik .idx istnieje (weryfikacja dostƒôpno≈õci)
        idx_available = False
        for server in servers:
            idx_url = f"https://{server}{idx_path}"
            try:
                idx_response = requests.head(idx_url, timeout=10, allow_redirects=True)
                if idx_response.status_code == 200:
                    idx_available = True
                    module_logger.debug(f"thr: {thread_id} - Plik .idx dostƒôpny na {server} dla f{forecast_hour:03d}")
                    break
            except:
                continue
        
        if not idx_available:
            module_logger.warning(f"thr: {thread_id} - Plik .idx niedostƒôpny dla f{forecast_hour:03d} (licznikProbPobrania = {attempt_count})")
        
        # Spr√≥buj pobraƒá z ka≈ºdego serwera po kolei
        for server in servers:
            url = f"https://{server}{base_path}"
            
            try:
                module_logger.info(f"thr: {thread_id} - Pobieranie (licznikProbPobrania = {attempt_count}): f{forecast_hour:03d}")
                # Pobierz plik
                response = requests.get(url, stream=True, timeout=300)
                status_code = response.status_code
                module_logger.info(f"thr: {thread_id} - Status pobrania pliku: {status_code}")
                
                if status_code == 200:
                    used_server = server
                    # Je≈õli uda≈Ço siƒô, przerwij pƒôtlƒô
                    break
                elif status_code == 404:
                    module_logger.warning(f"thr: {thread_id} - Plik f{forecast_hour:03d} niedostƒôpny na {server} (404)")
                    response.close()
                    response = None
                    continue
                else:
                    module_logger.warning(f"thr: {thread_id} - Nieoczekiwany status {status_code} z {server}")
                    response.close()
                    response = None
                    continue
            except requests.exceptions.Timeout:
                module_logger.warning(f"thr: {thread_id} - Timeout pobierania z {server} dla f{forecast_hour:03d}")
                if response:
                    response.close()
                response = None
                continue
            except requests.exceptions.RequestException as e:
                # Je≈õli b≈ÇƒÖd, spr√≥buj nastƒôpny serwer
                module_logger.warning(f"thr: {thread_id} - B≈ÇƒÖd pobierania z {server} dla f{forecast_hour:03d}: {e}")
                if response:
                    response.close()
                response = None
                continue
        
        # Je≈õli ≈ºaden serwer nie zadzia≈Ça≈Ç, zwr√≥ƒá b≈ÇƒÖd
        if response is None or response.status_code != 200:
            if attempt_count > 0:
                module_logger.warning(f"thr: {thread_id} - Pobieranie ponowne (licznikProbPobrania = {attempt_count}): f{forecast_hour:03d}")
            raise Exception(f"Nie uda≈Ço siƒô pobraƒá f{forecast_hour:03d} z ≈ºadnego serwera")
        
        # Zapisz tymczasowo
        if not os.path.exists('temp'):
            os.makedirs('temp')
        
        temp_file = os.path.join('temp', f'gfs_{self.run_date}_{self.run_hour}_f{forecast_hour:03d}.grib2')
        file_size_bytes = 0
        
        try:
            with open(temp_file, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024 * 1024):
                    if chunk:
                        f.write(chunk)
                        file_size_bytes += len(chunk)
            
            # Parsuj GRIB2
            all_datasets = []
            
            # Wycisz logi cfgrib i eccodes podczas parsowania
            cfgrib_logger = logging.getLogger('cfgrib')
            eccodes_logger = logging.getLogger('eccodes')
            ecmwf_logger = logging.getLogger('ecmwf')
            original_cfgrib_level = cfgrib_logger.level
            original_eccodes_level = eccodes_logger.level
            original_ecmwf_level = ecmwf_logger.level
            
            cfgrib_logger.setLevel(logging.ERROR)  # Tylko b≈Çƒôdy
            eccodes_logger.setLevel(logging.ERROR)  # Tylko b≈Çƒôdy
            ecmwf_logger.setLevel(logging.ERROR)  # Tylko b≈Çƒôdy
            
            # Wycisz r√≥wnie≈º root logger dla tych modu≈Ç√≥w
            root_logger = logging.getLogger()
            for handler in root_logger.handlers:
                if hasattr(handler, 'setLevel'):
                    # Nie zmieniamy poziomu handlera, tylko logger√≥w
                    pass
            
            try:
                module_logger.info(f"thr: {thread_id} - Rozpoczynam parsowanie GRIB2 dla f{forecast_hour:03d}")
                for idx, flt_cfg in enumerate(self.filters_config, 1):
                    try:
                        # St≈Çum b≈Çƒôdy ECCODES podczas parsowania
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore")
                            module_logger.debug(f"thr: {thread_id} - Parsowanie {flt_cfg['name']} ({idx}/{len(self.filters_config)}) dla f{forecast_hour:03d}")
                            ds = xr.open_dataset(
                                temp_file, 
                                engine='cfgrib',
                                backend_kwargs={
                                    'filter_by_keys': flt_cfg['filter'], 
                                    'indexpath': '',
                                    'errors': 'ignore'  # Ignoruj b≈Çƒôdy parsowania
                                }
                            )
                            
                            module_logger.debug(f"thr: {thread_id} - Wycinanie regionu dla {flt_cfg['name']} f{forecast_hour:03d}")
                            ds_region = ds.sel(
                                latitude=slice(self.lat_max, self.lat_min),
                                longitude=slice(self.lon_min, self.lon_max)
                            )
                            
                            all_datasets.append({
                                'name': flt_cfg['name'],
                                'dataset': ds_region,
                                'vars': flt_cfg['vars']
                            })
                            module_logger.debug(f"thr: {thread_id} - ‚úì {flt_cfg['name']} sparsowany dla f{forecast_hour:03d}")
                            
                    except Exception as e:
                        # Ignoruj b≈Çƒôdy parsowania - niekt√≥re pliki mogƒÖ mieƒá problemy
                        module_logger.debug(f"thr: {thread_id} - B≈ÇƒÖd parsowania {flt_cfg['name']} dla f{forecast_hour:03d}: {e}")
                        continue
            finally:
                # Przywr√≥ƒá oryginalny poziom logowania
                cfgrib_logger.setLevel(original_cfgrib_level)
                eccodes_logger.setLevel(original_eccodes_level)
                ecmwf_logger.setLevel(original_ecmwf_level)
                module_logger.info(f"thr: {thread_id} - Zako≈Ñczono parsowanie GRIB2 dla f{forecast_hour:03d} - znaleziono {len(all_datasets)} dataset√≥w")
            
            # Konwertuj do DataFrame
            if len(all_datasets) == 0:
                module_logger.warning(f"thr: {thread_id} - Brak dataset√≥w po parsowaniu f{forecast_hour:03d}")
                return (False, forecast_info, None, 0)
            
            module_logger.info(f"thr: {thread_id} - Konwertujƒô {len(all_datasets)} dataset√≥w do DataFrame dla f{forecast_hour:03d}")
            df = None
            
            for ds_info in all_datasets:
                ds = ds_info['dataset']
                level_name = ds_info['name']
                
                for var in ds_info['vars']:
                    if var not in ds.data_vars:
                        continue
                    
                    try:
                        data = ds[var]
                        
                        # Transformacje dla r√≥≈ºnych parametr√≥w
                        if var in ['t2m', 'd2m', 't']:
                            data = data - 273.15  # Konwersja z Kelvin na ¬∞C
                        elif var == 'prmsl':
                            data = data / 100  # Konwersja z Pa na hPa
                        elif var in ['tcc', 'lcc', 'mcc', 'hcc']:
                            data = data * 100  # Zachmurzenie z 0-1 na procenty 0-100
                        elif var == 'r2':
                            # r2 to wilgotno≈õƒá wzglƒôdna - zazwyczaj jest ju≈º w procentach w GRIB2
                            # Sprawd≈∫ czy trzeba przeliczyƒá (je≈õli warto≈õci sƒÖ w zakresie 0-1)
                            try:
                                max_val = float(data.max().values)
                                if max_val <= 1.0:
                                    data = data * 100  # Konwersja z 0-1 na procenty
                            except:
                                # Je≈õli nie mo≈ºna sprawdziƒá, zak≈Çadamy ≈ºe ju≈º w procentach
                                pass
                        elif var == 'prate':
                            # Intensywno≈õƒá opad√≥w - mo≈ºe potrzebowaƒá transformacji
                            # Prate jest w kg/m¬≤/s, mo≈ºna pozostawiƒá lub przeliczyƒá
                            pass
                        elif var in ['vis', 'dswrf']:
                            # Widzialno≈õƒá i promieniowanie - pozostawiamy jak sƒÖ
                            pass
                        
                        tmp = data.to_dataframe().reset_index()
                        coords = [c for c in ['latitude', 'longitude', 'time'] if c in tmp.columns]
                        
                        new_name = var
                        # Dodaj prefix dla kolizji nazw
                        if var in ['t', 'gh', 'u', 'v'] and level_name not in ['t2m', 'wind10']:
                            new_name = f"{var}_{level_name}"
                        
                        # Mapowanie nazw dla zgodno≈õci z bazƒÖ
                        if var == 'prmsl':
                            new_name = 'mslp'  # prmsl -> mslp
                        elif var == 'r2':
                            new_name = 'rh'  # r2 -> rh (wilgotno≈õƒá wzglƒôdna)
                        
                        if var in tmp.columns:
                            tmp.rename(columns={var: new_name}, inplace=True)
                        
                        cols = coords + [new_name]
                        tmp = tmp[cols]
                        
                        if df is None:
                            df = tmp
                        else:
                            df = df.merge(tmp, on=coords, how='outer')
                    
                    except:
                        continue
            
            # Zamknij datasets
            for ds_info in all_datasets:
                ds_info['dataset'].close()
            
            # Przygotuj DataFrame
            if df is None or len(df) == 0:
                return (False, forecast_info, None, 0)
            
            df['run_time'] = run_time
            df['created_at'] = datetime.utcnow()
            
            # WA≈ªNE: Nadpisz 'time' prawid≈Çowym forecast_time (run_time + forecast_hour)
            # Kolumna 'time' z GRIB2 mo≈ºe mieƒá nieprawid≈Çowe warto≈õci
            if 'time' in df.columns:
                # ZastƒÖp wszystkie warto≈õci 'time' prawid≈Çowym forecast_time
                df['time'] = forecast_time
            
            df.rename(columns={
                'latitude': 'lat',
                'longitude': 'lon',
                'time': 'forecast_time'
            }, inplace=True)
            
            # Oblicz wiatr
            if 'u10' in df.columns and 'v10' in df.columns:
                df['wind_speed'] = np.sqrt(df['u10']**2 + df['v10']**2)
                df['wind_dir'] = (270 - np.arctan2(df['v10'], df['u10']) * 180 / np.pi) % 360
            
            # ZaokrƒÖglij wszystkie kolumny numeryczne do 2 miejsc po przecinku
            # (opr√≥cz id - je≈õli istnieje)
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if col not in ['id']:  # Nie zaokrƒÖglaj ID je≈õli istnieje
                    # ZaokrƒÖglij do 2 miejsc po przecinku (zmniejsza rozmiar bazy)
                    df[col] = df[col].round(2)
            
            # ZAPIS DO CSV (backup przed zapisem do MySQL)
            csv_dir = os.path.join('temp', 'csv_backup')
            if not os.path.exists(csv_dir):
                os.makedirs(csv_dir)
            
            csv_file = os.path.join(csv_dir, f'gfs_{self.run_date}_{self.run_hour}_f{forecast_hour:03d}.csv')
            try:
                df.to_csv(csv_file, index=False, encoding='utf-8')
                module_logger.info(f"thr: {thread_id} - Zapisano do CSV: {len(df)} rekord√≥w dla f{forecast_hour:03d}")
            except Exception as e:
                module_logger.warning(f"thr: {thread_id} - B≈ÇƒÖd zapisu do CSV dla f{forecast_hour:03d}: {e}")
                # Kontynuuj mimo b≈Çƒôdu CSV
            
            # Zapis do bazy (na bie≈ºƒÖco)
            try:
                module_logger.info(f"thr: {thread_id} - Przeniesienie danych z CSV do bazy danych dla f{forecast_hour:03d}")
                df.to_sql(
                    "gfs_forecast",
                    self.engine,
                    if_exists="append",
                    index=False,
                    chunksize=1000,
                    method='multi'
                )
                file_size_mb = file_size_bytes / (1024 * 1024)
                module_logger.info(f"thr: {thread_id} - Zako≈Ñczono zapis do bazy dla f{forecast_hour:03d} ({len(df)} rekord√≥w, {file_size_mb:.2f} MB)")
            except Exception as e:
                # Mo≈ºliwe duplikaty - sprawd≈∫ przed zapisem
                module_logger.warning(f"thr: {thread_id} - B≈ÇƒÖd zapisu do bazy dla f{forecast_hour:03d}: {e}")
                return (False, forecast_info, None, 0)
            
            return (True, forecast_info, df, file_size_bytes)
            
        except Exception as e:
            module_logger.error(f"B≈ÇƒÖd pobierania/przetwarzania f{forecast_hour:03d}: {e}", exc_info=True)
            return (False, forecast_info, None, 0)
        
        finally:
            # Usu≈Ñ plik tymczasowy
            if temp_file and os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass

def worker_thread(queue, downloader, progress_queue, stats, thread_id=None):
    """WƒÖtek roboczy - pobiera prognozy z kolejki"""
    if thread_id is None:
        thread_id = threading.current_thread().ident
    
    while True:
        try:
            forecast_info = queue.get(timeout=1)
            
            if forecast_info is None:
                queue.task_done()
                break
            
            forecast_hour = forecast_info['forecast_hour']
            attempt_count = 0
            
            # Powiadom o rozpoczƒôciu przetwarzania
            module_logger.info(f"thr: {thread_id} - Rozpoczƒôto pobieranie f{forecast_hour:03d}")
            progress_queue.put({
                'type': 'start',
                'forecast_hour': forecast_hour,
                'thread_id': thread_id
            })
            
            # Pr√≥ba pobrania z ponawianiem
            success = False
            info = forecast_info
            df = None
            file_size_bytes = 0
            
            while attempt_count < 3:  # Maksymalnie 3 pr√≥by
                try:
                    success, info, df, file_size_bytes = downloader.download_and_process(forecast_info, progress_queue, thread_id, attempt_count)
                    if success:
                        break
                except Exception as e:
                    module_logger.warning(f"thr: {thread_id} - B≈ÇƒÖd pobierania f{forecast_hour:03d} (pr√≥ba {attempt_count + 1}): {e}")
                    if attempt_count < 2:
                        module_logger.info(f"thr: {thread_id} - Czekam 2 min. na pobranie pliku...")
                        time.sleep(120)  # Czekaj 2 minuty przed ponownƒÖ pr√≥bƒÖ
                
                attempt_count += 1
            
            if success:
                stats['success'] += 1
                stats['total_records'] += len(df) if df is not None else 0
                if 'total_bytes' not in stats:
                    stats['total_bytes'] = 0
                stats['total_bytes'] += file_size_bytes
            else:
                stats['failed'] += 1
                module_logger.error(f"thr: {thread_id} - Nie uda≈Ço siƒô pobraƒá f{forecast_hour:03d} po {attempt_count} pr√≥bach")
            
            # Powiadom o zako≈Ñczeniu przetwarzania
            progress_queue.put({
                'type': 'done',
                'forecast_hour': info['forecast_hour'],
                'success': success,
                'total_records': len(df) if df is not None else 0,
                'file_size_bytes': file_size_bytes,
                'thread_id': thread_id
            })
            
            queue.task_done()
            
        except Empty:
            continue
        except Exception as e:
            module_logger.error(f"thr: {thread_id} - B≈ÇƒÖd w worker_thread: {e}", exc_info=True)
            stats['failed'] += 1
            # Spr√≥buj oznaczyƒá zadanie jako zako≈Ñczone, je≈õli to mo≈ºliwe
            try:
                queue.task_done()
            except:
                pass

    # === 8. URUCHOMIENIE POBRANIA Z AUTOMATYCZNYM PONAWIANIEM ===
    if _is_main_module:
        print(f"\n‚è≥ Rozpoczynam pobieranie prognoz...")
        print(f"  U≈ºywam {NUM_THREADS} wƒÖtk√≥w r√≥wnolegle")
        print(f"  Priorytet: naj≈õwie≈ºsze pierwsze (f000, f001, f002...)")
        print(f"  Automatyczne ponawianie: 30s miƒôdzy pr√≥bami")
        print(f"  System bƒôdzie kontynuowa≈Ç a≈º wszystkie 209 prognoz bƒôdƒÖ pobrane")
        print(f"  (Naci≈õnij Ctrl+C aby przerwaƒá)\n")

        # Rozpocznij pomiar czasu
        start_time = time.time()

        # Stw√≥rz downloader
        downloader = ForecastDownloader(RUN_DATE, RUN_HOUR, lat_min, lat_max, lon_min, lon_max, engine)

        # Pƒôtla automatycznego ponawiania
        attempt = 1
        total_success = 0
        total_failed = 0
        total_records = 0
        WAIT_BETWEEN_ATTEMPTS = 30  # sekund

        while True:
            try:
                # Sprawd≈∫ kt√≥re prognozy jeszcze brakujƒÖ
                existing_hours = get_existing_forecast_hours(run_time, engine)
                required_hours = get_required_forecast_hours()
                missing_hours = sorted(list(required_hours - existing_hours))
                
                if len(missing_hours) == 0:
                    print(f"\n‚úì‚úì‚úì Wszystkie 209 prognoz sƒÖ ju≈º pobrane!")
                    break
                
                # Je≈õli to nie pierwsza pr√≥ba, poka≈º status
                if attempt > 1:
                    print(f"\n{'='*70}")
                    print(f"üîÑ Pr√≥ba #{attempt} - brakuje jeszcze {len(missing_hours)} prognoz")
                    print(f"{'='*70}")
                
                # Filtruj prognozy do pobrania (tylko te kt√≥re brakujƒÖ)
                forecasts_to_download_this_round = [
                    f for f in all_forecasts 
                    if f['forecast_hour'] in missing_hours
                ]
                
                if len(forecasts_to_download_this_round) == 0:
                    break
                
                print(f"\n‚è≥ Pr√≥ba #{attempt}: Pobieranie {len(forecasts_to_download_this_round)} brakujƒÖcych prognoz...")
                
                # Przygotuj kolejki i statystyki dla tej rundy
                download_queue = queue.Queue()
                progress_queue = queue.Queue()
                stats = {'success': 0, 'failed': 0, 'total_records': 0}
                currently_processing = set()
                last_completed = None
                
                # Dodaj prognozy do kolejki
                for forecast in forecasts_to_download_this_round:
                    download_queue.put(forecast)
                
                # Uruchom wƒÖtki
                threads = []
                for i in range(NUM_THREADS):
                    t = threading.Thread(target=worker_thread, args=(download_queue, downloader, progress_queue, stats, i+1))
                    t.daemon = True
                    t.start()
                    threads.append(t)
                
                # Progress bar dla tej rundy
                with tqdm(total=len(forecasts_to_download_this_round), desc=f"Runda #{attempt}", unit="prognoz", 
                          bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}') as pbar:
                    processed = 0
                    
                    while processed < len(forecasts_to_download_this_round):
                        try:
                            progress = progress_queue.get(timeout=30)
                            
                            if progress.get('type') == 'start':
                                forecast_hour = progress['forecast_hour']
                                currently_processing.add(forecast_hour)
                                
                                processing_list = sorted(list(currently_processing))[:5]
                                processing_str = ', '.join([f"f{h:03d}" for h in processing_list])
                                if len(currently_processing) > 5:
                                    processing_str += f" +{len(currently_processing) - 5}"
                                
                                pbar.set_postfix({
                                    'Teraz': processing_str if processing_str else '-',
                                    'OK': stats['success'],
                                    'FAIL': stats['failed']
                                })
                                
                            elif progress.get('type') == 'done':
                                forecast_hour = progress['forecast_hour']
                                currently_processing.discard(forecast_hour)
                                last_completed = forecast_hour
                                processed += 1
                                
                                processing_list = sorted(list(currently_processing))[:5]
                                processing_str = ', '.join([f"f{h:03d}" for h in processing_list])
                                if len(currently_processing) > 5:
                                    processing_str += f" +{len(currently_processing) - 5}"
                                
                                last_str = f"f{forecast_hour:03d}" if last_completed is not None else '-'
                                
                                if progress['success']:
                                    pbar.set_postfix({
                                        'Teraz': processing_str if processing_str else '-',
                                        'Ostatnia': last_str,
                                        'OK': stats['success'],
                                        'FAIL': stats['failed']
                                    })
                                else:
                                    pbar.set_postfix({
                                        'Teraz': processing_str if processing_str else '-',
                                        'Ostatnia': f"{last_str} (ERROR)",
                                        'OK': stats['success'],
                                        'FAIL': stats['failed']
                                    })
                                
                                pbar.update(1)
                            
                        except Empty:
                            alive = sum(1 for t in threads if t.is_alive())
                            if alive == 0:
                                break
                
                # Poczekaj na zako≈Ñczenie wszystkich wƒÖtk√≥w
                for t in threads:
                    t.join(timeout=5)
                
                # Dodaj None do kolejki aby zako≈Ñczyƒá wƒÖtki
                for _ in range(NUM_THREADS):
                    download_queue.put(None)
                
                # Zaktualizuj statystyki ca≈Çkowite
                total_success += stats['success']
                total_failed += stats['failed']
                total_records += stats['total_records']
                
                # Sprawd≈∫ czy wszystkie sƒÖ ju≈º pobrane
                existing_hours_after = get_existing_forecast_hours(run_time, engine)
                missing_hours_after = sorted(list(required_hours - existing_hours_after))
                
                if len(missing_hours_after) == 0:
                    print(f"\n‚úì‚úì‚úì Wszystkie 209 prognoz sƒÖ ju≈º pobrane!")
                    break
                
                # Je≈õli nie ma nowych sukces√≥w, sprawd≈∫ czy warto kontynuowaƒá
                if stats['success'] == 0:
                    # Sprawd≈∫ dostƒôpno≈õƒá od najni≈ºszej brakujƒÖcej (dane sƒÖ tworzone sukcesywnie)
                    # Je≈õli nie ma f080, to na pewno nie ma te≈º f081, f082 itd.
                    min_missing = min(missing_hours_after) if missing_hours_after else None
                    is_available = False
                    
                    if min_missing is not None:
                        # Sprawd≈∫ kilka najni≈ºszych brakujƒÖcych prognoz
                        check_hours = sorted(missing_hours_after)[:5]  # Sprawd≈∫ pierwsze 5
                        for hour in check_hours:
                            if check_gfs_availability(RUN_DATE, RUN_HOUR, hour):
                                is_available = True
                                print(f"\n‚úì Prognoza f{hour:03d} jest dostƒôpna online")
                                break
                    
                    if not is_available:
                        # Brak dostƒôpnych prognoz - poczekaj i spr√≥buj ponownie
                        if min_missing is not None:
                            print(f"\n‚è≥ Najni≈ºsza brakujƒÖca prognoza: f{min_missing:03d} - jeszcze niedostƒôpna")
                        print(f"‚è≥ Czekam {WAIT_BETWEEN_ATTEMPTS}s przed nastƒôpnƒÖ pr√≥bƒÖ...")
                        print(f"   (Naci≈õnij Ctrl+C aby przerwaƒá)")
                        time.sleep(WAIT_BETWEEN_ATTEMPTS)
                    else:
                        # SƒÖ dostƒôpne prognozy, ale nie uda≈Ço siƒô ich pobraƒá - kontynuuj
                        print(f"\n‚è≥ Czekam {WAIT_BETWEEN_ATTEMPTS}s przed nastƒôpnƒÖ pr√≥bƒÖ...")
                        print(f"   (Naci≈õnij Ctrl+C aby przerwaƒá)")
                        time.sleep(WAIT_BETWEEN_ATTEMPTS)
                else:
                    # By≈Çy sukcesy - kontynuuj od razu (dane sƒÖ tworzone sukcesywnie)
                    print(f"\n‚úì Pobrano {stats['success']} prognoz. Kontynuujƒô...")
                    time.sleep(2)  # Kr√≥tka przerwa przed nastƒôpnƒÖ pr√≥bƒÖ
                
                attempt += 1
                
            except KeyboardInterrupt:
                print(f"\n\n‚ö†Ô∏è  Przerwano przez u≈ºytkownika (Ctrl+C)")
                print(f"   Pobrano ≈ÇƒÖcznie: {total_success} prognoz w {attempt-1} pr√≥bach")
                break
            except Exception as e:
                print(f"\n‚ö†Ô∏è  B≈ÇƒÖd podczas pobierania: {e}")
                print(f"   Czekam {WAIT_BETWEEN_ATTEMPTS}s przed nastƒôpnƒÖ pr√≥bƒÖ...")
                time.sleep(WAIT_BETWEEN_ATTEMPTS)
                attempt += 1

        # Zako≈Ñcz pomiar czasu
        end_time = time.time()
        elapsed_time = end_time - start_time
        hours = int(elapsed_time // 3600)
        minutes = int((elapsed_time % 3600) // 60)
        seconds = int(elapsed_time % 60)

        # Formatuj czas
        if hours > 0:
            time_str = f"{hours}h {minutes}m {seconds}s"
        elif minutes > 0:
            time_str = f"{minutes}m {seconds}s"
        else:
            time_str = f"{seconds}s"

        # === 9. PODSUMOWANIE ===
        print("\n" + "=" * 70)
        print("‚úì‚úì‚úì POBRANIE ZAKO≈ÉCZONE!")
        print("=" * 70)
        print(f"Run GFS:          {run_time.strftime('%Y-%m-%d %H:00')} UTC")
        print(f"Pr√≥b wykonano:     {attempt-1}")
        print(f"Prognoz pobrano:   {total_success}")
        print(f"Prognoz b≈Çƒôd√≥w:    {total_failed}")
        print(f"Rekord√≥w w bazie:  {total_records}")
        print(f"‚è±Ô∏è  Czas pobrania:   {time_str} ({elapsed_time:.1f} sekund)")
        print("=" * 70)

        # Sprawd≈∫ ko≈Ñcowy stan
        try:
            with engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT COUNT(DISTINCT forecast_time) as count
                    FROM gfs_forecast
                    WHERE run_time = :run_time
                """), {"run_time": run_time})
            row = result.fetchone()
            if row:
                final_count = row[0]
                print(f"\n‚úì Ko≈Ñcowa liczba prognoz w bazie: {final_count}")
            
        except Exception as e:
            print(f"\n‚ö† Nie uda≈Ço siƒô sprawdziƒá ko≈Ñcowej liczby prognoz: {e}")
            pass

        # === 10. KASOWANIE STARYCH RUN√ìW (zostaw tylko 2 ostatnie) ===
        if total_success > 0:  # Tylko je≈õli uda≈Ço siƒô pobraƒá co≈õ
            print(f"\n‚è≥ Czyszczenie starych run√≥w (zostaw tylko 2 ostatnie)...")
            
            try:
                with engine.connect() as conn:
                    # Znajd≈∫ wszystkie run_time w bazie
                    result = conn.execute(text("""
                        SELECT DISTINCT run_time
                        FROM gfs_forecast
                        ORDER BY run_time DESC
                    """))
                
                all_runs = [row[0] for row in result.fetchall()]
                
                if len(all_runs) > 2:
                    # Zachowaj tylko 2 najnowsze runy
                    runs_to_keep = sorted(all_runs, reverse=True)[:2]
                    runs_to_delete = [rt for rt in all_runs if rt not in runs_to_keep]
                    
                    if runs_to_delete:
                        # Usu≈Ñ stare runy (sprzed 2 ostatnich)
                        for old_run in runs_to_delete:
                            delete_result = conn.execute(text("""
                                DELETE FROM gfs_forecast
                                WHERE run_time = :old_run
                            """), {"old_run": old_run})
                            deleted_count = delete_result.rowcount
                            print(f"  ‚úì Usuniƒôto run {old_run.strftime('%Y-%m-%d %H:00')}: {deleted_count} rekord√≥w")
                        
                        conn.commit()
                        
                        if len(runs_to_delete) > 0:
                            print(f"‚úì Usuniƒôto {len(runs_to_delete)} starych run(√≥w)")
                            print(f"  Zosta≈Çy tylko 2 najnowsze runy w bazie")
                else:
                    print(f"‚úì W bazie jest {len(all_runs)} run(√≥w) - wszystko OK")
                    
            except Exception as e:
                print(f"‚ö† B≈ÇƒÖd podczas czyszczenia starych run√≥w: {e}")
                import traceback
                traceback.print_exc()

        print("\nüí° Wszystkie dane sƒÖ ju≈º zapisane w bazie!")
        print(f"   Tabela: gfs_forecast")
        print(f"   Baza: {MYSQL_DATABASE}")

        input("\nNaci≈õnij Enter...")


"""
GFS Weather Data Downloader - FILTERED VERSION
Pobiera tylko wybrane parametry z GRIB Filter API - OSZCZƒòDZA ~85-90% PRZEPUSTOWO≈öCI!
Pe≈Çny zakres prognoz: f000-f120 (co 1h) + f123-f384 (co 3h) = 209 prognoz
Z multi-threading, resume, progress bar i priorytetyzacjƒÖ + FILTROWANIE PARAMETR√ìW
"""

import xarray as xr
import pandas as pd
import numpy as np
import requests
import os
import configparser
from datetime import datetime, timedelta
from sqlalchemy import create_engine, text
import threading
import queue
from queue import Empty
import time
from tqdm import tqdm
import warnings
import os
import logging
from collections import deque
from urllib.parse import urlencode
warnings.filterwarnings('ignore')

# St≈Çum b≈Çƒôdy ECCODES (sƒÖ tylko ostrze≈ºeniami)
os.environ['ECCODES_LOG_VERBOSITY'] = '0'
os.environ['ECCODES_DEBUG'] = '0'

# Wycisz logi DEBUG z cfgrib, ecmwf, eccodes, urllib3, requests
logging.getLogger('cfgrib').setLevel(logging.WARNING)
logging.getLogger('ecmwf').setLevel(logging.WARNING)
logging.getLogger('eccodes').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('urllib3.connectionpool').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)

# Logger dla modu≈Çu
module_logger = logging.getLogger(__name__)

# === KONFIGURACJA PARAMETR√ìW DO FILTROWANIA ===
# Wybierz tylko te parametry, kt√≥rych potrzebujesz!
# Lista wszystkich dostƒôpnych parametr√≥w: https://www.nco.ncep.noaa.gov/pmb/products/gfs/

GRIB_FILTER_CONFIG = {
    # === POZIOMY IZOBARYCZNE (mb) ===
    # Standardowe poziomy dla prognoz pogody
    'levels': [
        '1000_mb', '975_mb', '950_mb', '925_mb', '900_mb',  # Przypowierzchniowe
        '850_mb', '800_mb', '700_mb',                        # Dolna troposfera
        '500_mb', '400_mb', '300_mb', '250_mb', '200_mb',    # ≈örodkowa/g√≥rna troposfera
        '150_mb', '100_mb', '50_mb'                          # Stratosfera (opcjonalnie)
    ],
    
    # === ZMIENNE ATMOSFERYCZNE ===
    'variables': [
        # Podstawowe pola
        'HGT',      # Wysoko≈õƒá geopotencjalna [gpm]
        'TMP',      # Temperatura [K]
        'RH',       # Wilgotno≈õƒá wzglƒôdna [%]
        'UGRD',     # Sk≈Çadowa U wiatru [m/s]
        'VGRD',     # Sk≈Çadowa V wiatru [m/s]
        
        # Dodatkowe pola (odkomentuj je≈õli potrzebujesz)
        'VVEL',     # Prƒôdko≈õƒá wertykalna (ci≈õnienie) [Pa/s]
        # 'DZDT',     # Prƒôdko≈õƒá wertykalna (geometryczna) [m/s]
        # 'ABSV',     # Wirowanie absolutne [1/s]
        # 'SPFH',     # Wilgotno≈õƒá w≈Ça≈õciwa [kg/kg]
        
        # Chmury i opady
        'TCDC',     # Zachmurzenie ca≈Çkowite [%]
        'CLWMR',    # Cloud mixing ratio [kg/kg]
        'ICMR',     # Ice water mixing ratio [kg/kg]
        # 'RWMR',     # Rain mixing ratio [kg/kg]
        # 'SNMR',     # Snow mixing ratio [kg/kg]
        # 'GRLE',     # Graupel [kg/kg]
        
        # Ozon (dla analizy stratosferycznej)
        # 'O3MR',     # Ozone mixing ratio [kg/kg]
    ],
    
    # === POWIERZCHNIA I WARSTWY SPECJALNE ===
    'surface_levels': [
        'surface',              # Powierzchnia
        'mean_sea_level',       # Poziom morza
        '2_m_above_ground',     # 2m nad ziemiƒÖ (temp, wilgotno≈õƒá)
        '10_m_above_ground',    # 10m nad ziemiƒÖ (wiatr)
        '80_m_above_ground',    # 80m nad ziemiƒÖ (turbiny wiatrowe)
        '100_m_above_ground',   # 100m nad ziemiƒÖ
        'tropopause',           # Tropopauza
        'max_wind',             # Max wind level
        # 'entire_atmosphere',  # Ca≈Ça atmosfera (PWAT, CWAT)
        # 'planetary_boundary_layer',
    ],
    
    # === ZMIENNE POWIERZCHNIOWE ===
    'surface_variables': [
        'PRES',     # Ci≈õnienie na powierzchni [Pa]
        'PRMSL',    # Ci≈õnienie na poziomie morza [Pa]
        'TMP',      # Temperatura powierzchni [K]
        'DPT',      # Punkt rosy [K]
        'RH',       # Wilgotno≈õƒá wzglƒôdna [%]
        'UGRD',     # Sk≈Çadowe wiatru [m/s]
        'VGRD',
        'GUST',     # Porywy wiatru [m/s]
        
        # Opady i chmury
        'APCP',     # Accumulated precipitation [kg/m^2]
        'CAPE',     # Convective available potential energy [J/kg]
        'CIN',      # Convective inhibition [J/kg]
        'PWAT',     # Precipitable water [kg/m^2]
        
        # ≈önieg i gleba
        'WEASD',    # Water equivalent of accumulated snow depth [kg/m^2]
        'SNOD',     # Snow depth [m]
        'TSOIL',    # Soil temperature [K]
        'SOILW',    # Soil moisture [Fraction]
        
        # Dodatkowe (odkomentuj je≈õli potrzebujesz)
        # 'VIS',      # Visibility [m]
        # 'HINDEX',   # Haines index
        # 'LFTX',     # Lifted index [K]
        # 'SUNSD',    # Sunshine duration [s]
        # 'ICEC',     # Ice cover [Proportion]
    ],
}

# === RATE LIMITING - 120 zapyta≈Ñ/minutƒô ===
_rate_limit_lock = threading.Lock()
_rate_limit_timestamps = deque(maxlen=120)

def wait_for_rate_limit():
    """
    Czeka je≈õli potrzeba, ≈ºeby nie przekroczyƒá limitu 120 zapyta≈Ñ/minutƒô.
    Thread-safe.
    """
    global _rate_limit_timestamps
    
    with _rate_limit_lock:
        now = time.time()
        
        # Usu≈Ñ stare timestampy (starsze ni≈º 60 sekund)
        while _rate_limit_timestamps and (now - _rate_limit_timestamps[0]) > 60:
            _rate_limit_timestamps.popleft()
        
        # Je≈õli mamy ju≈º 120 zapyta≈Ñ w ostatniej minucie, poczekaj
        if len(_rate_limit_timestamps) >= 120:
            oldest_timestamp = _rate_limit_timestamps[0]
            wait_time = 60 - (now - oldest_timestamp) + 0.1
            if wait_time > 0:
                module_logger.debug(f"Rate limit: czekam {wait_time:.2f}s (120 zapyta≈Ñ/min)")
                time.sleep(wait_time)
                now = time.time()
                while _rate_limit_timestamps and (now - _rate_limit_timestamps[0]) > 60:
                    _rate_limit_timestamps.popleft()
        
        # Dodaj aktualny timestamp
        _rate_limit_timestamps.append(time.time())
        
        # Minimalne op√≥≈∫nienie miƒôdzy zapytaniami (0.5s = 120/min)
        time.sleep(0.5)

def build_grib_filter_url(date_str, hour_str, forecast_hour, resolution='0p25'):
    """
    Buduje URL dla GRIB Filter API z wybranymi parametrami.
    
    Args:
        date_str: Data w formacie YYYYMMDD
        hour_str: Godzina cyklu (00, 06, 12, 18)
        forecast_hour: Godzina prognozy (0-384)
        resolution: Rozdzielczo≈õƒá (0p25, 0p50, 1p00)
    
    Returns:
        str: Pe≈Çny URL do GRIB Filter API
    """
    base_url = f"https://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_{resolution}.pl"
    
    # Parametry URL
    params = {
        'file': f'gfs.t{hour_str}z.pgrb2.{resolution}.f{forecast_hour:03d}',
        'dir': f'/gfs.{date_str}/{hour_str}/atmos',
    }
    
    # Dodaj poziomy izobaryczne
    for level in GRIB_FILTER_CONFIG['levels']:
        params[f'lev_{level}'] = 'on'
    
    # Dodaj poziomy powierzchniowe
    for level in GRIB_FILTER_CONFIG['surface_levels']:
        params[f'lev_{level}'] = 'on'
    
    # Dodaj zmienne atmosferyczne (dla poziom√≥w izobarycznych)
    for var in GRIB_FILTER_CONFIG['variables']:
        params[f'var_{var}'] = 'on'
    
    # Dodaj zmienne powierzchniowe
    for var in GRIB_FILTER_CONFIG['surface_variables']:
        params[f'var_{var}'] = 'on'
    
    # Opcjonalnie: subregion (przyspiesza pobieranie i zmniejsza rozmiar)
    # Odkomentuj poni≈ºsze linie je≈õli chcesz ograniczyƒá region
    # params['subregion'] = ''
    # params['leftlon'] = '0'      # Minimalna d≈Çugo≈õƒá geograficzna
    # params['rightlon'] = '360'   # Maksymalna d≈Çugo≈õƒá geograficzna
    # params['toplat'] = '90'      # Maksymalna szeroko≈õƒá geograficzna
    # params['bottomlat'] = '-90'  # Minimalna szeroko≈õƒá geograficzna
    
    return f"{base_url}?{urlencode(params)}"

def download_grib_filtered(url, output_path, max_retries=3):
    """
    Pobiera plik GRIB u≈ºywajƒÖc GRIB Filter API.
    Zwraca (success, file_size_bytes).
    """
    for attempt in range(max_retries):
        try:
            # Rate limiting
            wait_for_rate_limit()
            
            # Pobierz plik
            response = requests.get(url, timeout=60, stream=True)
            
            # Obs≈Çuga HTTP 429 (Too Many Requests)
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', 60))
                module_logger.warning(f"HTTP 429 - czekam {retry_after}s")
                time.sleep(retry_after)
                continue
            
            if response.status_code != 200:
                module_logger.debug(f"HTTP {response.status_code} dla {url}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                return False, 0
            
            # Zapisz plik
            file_size = 0
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        file_size += len(chunk)
            
            # Sprawd≈∫ czy plik nie jest pusty
            if file_size < 1024:  # Mniej ni≈º 1KB = prawdopodobnie b≈ÇƒÖd
                if os.path.exists(output_path):
                    os.remove(output_path)
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return False, 0
            
            return True, file_size
            
        except requests.exceptions.Timeout:
            module_logger.debug(f"Timeout (attempt {attempt+1}/{max_retries})")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            continue
        except Exception as e:
            module_logger.debug(f"B≈ÇƒÖd pobierania: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            continue
    
    return False, 0

# === RESZTA FUNKCJI Z PROFESSIONAL VERSION (bez zmian) ===

def check_gfs_availability(date_str, hour_str, forecast_hour, verbose=False):
    """
    Sprawdza czy dana prognoza GFS jest dostƒôpna.
    Dla wersji filtered sprawdza dostƒôpno≈õƒá przez GRIB Filter API.
    """
    # Dla filtered u≈ºywamy GRIB Filter URL
    url = build_grib_filter_url(date_str, hour_str, forecast_hour)
    
    try:
        wait_for_rate_limit()
        response = requests.head(url, timeout=10, allow_redirects=True)
        
        if response.status_code == 429:
            retry_after = int(response.headers.get('Retry-After', 60))
            if verbose:
                module_logger.debug(f"HTTP 429 - czekam {retry_after}s")
            time.sleep(retry_after)
            wait_for_rate_limit()
            response = requests.head(url, timeout=10, allow_redirects=True)
        
        if response.status_code == 200:
            if verbose:
                module_logger.debug(f"‚úì Dane dostƒôpne (f{forecast_hour:03d})")
            return True
            
    except requests.exceptions.Timeout:
        if verbose:
            module_logger.debug(f"Timeout sprawdzania dostƒôpno≈õci")
    except Exception as e:
        if verbose:
            module_logger.debug(f"B≈ÇƒÖd sprawdzania: {e}")
    
    return False

def get_required_forecast_hours():
    """
    Zwraca set wymaganych forecast_hour do pobrania:
    - f000-f120 (co 1h) = 121 prognoz
    - f123-f384 (co 3h) = 88 prognoz
    RAZEM: 209 prognoz
    """
    required_hours = set()
    
    # f000-f120: co 1h
    for hour in range(0, 121):
        required_hours.add(hour)
    
    # f123-f384: co 3h
    for hour in range(123, 385, 3):
        required_hours.add(hour)
    
    return required_hours

def get_existing_forecast_hours(run_time, engine=None):
    """
    Zwraca set forecast_hour kt√≥re sƒÖ ju≈º w bazie dla danego run_time.
    """
    if engine is None:
        try:
            engine = globals().get('engine')
            if engine is None:
                return set()
        except:
            return set()
    
    try:
        with engine.connect() as conn:
            run_time_str = run_time.strftime('%Y-%m-%d %H:%M:%S')
            
            result = conn.execute(text("""
                SELECT DISTINCT forecast_time
                FROM gfs_forecast
                WHERE DATE_FORMAT(run_time, '%Y-%m-%d %H:%i:%s') = :run_time
                ORDER BY forecast_time
            """), {"run_time": run_time_str})
            
            existing_hours = set()
            rows = result.fetchall()
            
            for row in rows:
                forecast_time = row[0]
                
                if isinstance(forecast_time, str):
                    try:
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M']:
                            try:
                                forecast_time = datetime.strptime(forecast_time, fmt)
                                break
                            except:
                                continue
                    except:
                        continue
                
                if isinstance(forecast_time, datetime):
                    time_diff = forecast_time - run_time
                    forecast_hour = int(time_diff.total_seconds() / 3600)
                    existing_hours.add(forecast_hour)
            
            return existing_hours
            
    except Exception as e:
        print(f"‚ö† B≈ÇƒÖd sprawdzania forecast_hour w bazie: {e}")
        return set()

def find_latest_gfs_run(engine=None):
    """Znajduje najnowszy dostƒôpny run GFS (szuka nowszego ni≈º w bazie)"""
    if engine is None:
        try:
            engine = globals().get('engine')
            if engine is None:
                return None, None, None
        except:
            return None, None, None
    
    now_utc = datetime.utcnow()
    current_run_hour = (now_utc.hour // 6) * 6
    run_time = now_utc.replace(hour=current_run_hour, minute=0, second=0, microsecond=0)
    
    required_hours = get_required_forecast_hours()
    
    last_run_in_db = None
    try:
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT MAX(run_time) as last_run
                FROM gfs_forecast
            """))
            row = result.fetchone()
            if row and row[0]:
                last_run_in_db = row[0]
                if isinstance(last_run_in_db, str):
                    try:
                        last_run_in_db = datetime.strptime(last_run_in_db, '%Y-%m-%d %H:%M:%S')
                    except:
                        try:
                            last_run_in_db = datetime.strptime(last_run_in_db, '%Y-%m-%d %H:%M')
                        except:
                            pass
    except:
        pass
    
    found_run = None
    
    for i in range(6):
        check_time = run_time - timedelta(hours=i * 6)
        date_str = check_time.strftime("%Y%m%d")
        hour_str = f"{check_time.hour:02d}"
        
        if last_run_in_db and check_time <= last_run_in_db:
            continue
        
        # Sprawd≈∫ dostƒôpno≈õƒá pierwszej prognozy (f000)
        if check_gfs_availability(date_str, hour_str, 0):
            found_run = check_time
            break
    
    if found_run:
        date_str = found_run.strftime("%Y%m%d")
        hour_str = f"{found_run.hour:02d}"
        return found_run, date_str, hour_str
    
    return None, None, None

def process_grib_to_db_filtered(grib_path, run_time, forecast_hour, lat_min, lat_max, lon_min, lon_max, engine):
    """
    Przetwarza plik GRIB (pofiltrowany) i zapisuje do bazy danych.
    Identyczna funkcjonalno≈õƒá jak w professional, ale dla filtrowanych plik√≥w.
    """
    try:
        # Otw√≥rz plik GRIB
        ds = xr.open_dataset(grib_path, engine='cfgrib', backend_kwargs={
            'indexpath': '',
            'errors': 'ignore'
        })
        
        # Filtruj region geograficzny
        ds_region = ds.sel(
            latitude=slice(lat_max, lat_min),
            longitude=slice(lon_min, lon_max)
        )
        
        # Oblicz forecast_time
        forecast_time = run_time + timedelta(hours=int(forecast_hour))
        
        # Przygotuj dane do zapisu
        records = []
        
        for lat_idx in range(len(ds_region.latitude)):
            for lon_idx in range(len(ds_region.longitude)):
                lat = float(ds_region.latitude.values[lat_idx])
                lon = float(ds_region.longitude.values[lon_idx])
                
                record = {
                    'run_time': run_time,
                    'forecast_time': forecast_time,
                    'latitude': lat,
                    'longitude': lon
                }
                
                # Zbierz wszystkie zmienne
                for var_name in ds_region.data_vars:
                    try:
                        var_data = ds_region[var_name]
                        
                        # Sprawd≈∫ czy zmienna ma wymiary lat/lon
                        if 'latitude' in var_data.dims and 'longitude' in var_data.dims:
                            value = float(var_data.values[lat_idx, lon_idx])
                            
                            if not np.isnan(value) and not np.isinf(value):
                                # Mapuj nazwƒô zmiennej na format bazy
                                level = ''
                                if 'isobaricInhPa' in var_data.dims:
                                    level_val = var_data.coords['isobaricInhPa'].values
                                    if isinstance(level_val, np.ndarray):
                                        level_val = level_val.item() if level_val.size == 1 else level_val[0]
                                    level = f"_{int(level_val)}_mb"
                                elif 'heightAboveGround' in var_data.dims:
                                    height_val = var_data.coords['heightAboveGround'].values
                                    if isinstance(height_val, np.ndarray):
                                        height_val = height_val.item() if height_val.size == 1 else height_val[0]
                                    level = f"_{int(height_val)}_m"
                                
                                db_col_name = f"{var_name}{level}".lower()
                                record[db_col_name] = value
                    except:
                        continue
                
                if len(record) > 4:  # Je≈õli sƒÖ jakie≈õ dane poza podstawowymi polami
                    records.append(record)
        
        # Zapisz do bazy
        if records:
            df = pd.DataFrame(records)
            df.to_sql('gfs_forecast', engine, if_exists='append', index=False, method='multi', chunksize=1000)
            return len(records)
        else:
            return 0
            
    except Exception as e:
        module_logger.error(f"B≈ÇƒÖd przetwarzania GRIB: {e}")
        return 0

# === G≈Å√ìWNY KOD ===
try:
    import builtins
    _is_imported_by_daemon = hasattr(builtins, '__imported_by_daemon__')
except:
    _is_imported_by_daemon = False

_is_main_module = (__name__ == "__main__" and not _is_imported_by_daemon)
if _is_main_module:
    print("=" * 70)
    print("GFS Weather Data Downloader - FILTERED VERSION")
    print("üéØ FILTROWANIE: Pobiera tylko wybrane parametry (~85-90% oszczƒôdno≈õci!)")
    print("=" * 70)
    
    # Poka≈º konfiguracjƒô filtr√≥w
    print(f"\nüìã KONFIGURACJA FILTROWANIA:")
    print(f"  Poziomy izobaryczne: {len(GRIB_FILTER_CONFIG['levels'])} poziom√≥w")
    print(f"  Zmienne atmosferyczne: {len(GRIB_FILTER_CONFIG['variables'])} zmiennych")
    print(f"  Poziomy powierzchniowe: {len(GRIB_FILTER_CONFIG['surface_levels'])} poziom√≥w")
    print(f"  Zmienne powierzchniowe: {len(GRIB_FILTER_CONFIG['surface_variables'])} zmiennych")
    print(f"\nüí° Edytuj GRIB_FILTER_CONFIG w pliku aby zmieniƒá parametry")
    
    # === 1. KONFIGURACJA ===
    try:
        config = configparser.ConfigParser()
        config.read("config.ini", encoding='utf-8')
        
        MYSQL_USER = config["database"]["user"]
        MYSQL_PASSWORD = config["database"]["password"]
        MYSQL_HOST = config["database"]["host"]
        MYSQL_DATABASE = config["database"]["database"]
        
        lat_min = float(config["region"]["lat_min"])
        lat_max = float(config["region"]["lat_max"])
        lon_min = float(config["region"]["lon_min"])
        lon_max = float(config["region"]["lon_max"])
        
        NUM_THREADS = 6
        
        print(f"\n‚úì Konfiguracja OK")
        print(f"  Region: {lat_min}¬∞-{lat_max}¬∞N, {lon_min}¬∞-{lon_max}¬∞E")
        print(f"  WƒÖtki: {NUM_THREADS}")
        
    except Exception as e:
        print(f"‚úó B≈ÅƒÑD konfiguracji: {e}")
        input("\nEnter...")
        exit(1)
    
    # === 2. PO≈ÅƒÑCZENIE Z BAZƒÑ ===
    try:
        print(f"\n‚è≥ ≈ÅƒÖczenie z MySQL...")
        
        MYSQL_URL = f"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}/{MYSQL_DATABASE}?charset=utf8mb4"
        engine = create_engine(MYSQL_URL, echo=False, pool_pre_ping=True)
        
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        
        print(f"‚úì MySQL OK: {MYSQL_DATABASE}")
        
    except Exception as e:
        print(f"‚úó B≈ÅƒÑD MySQL: {e}")
        input("\nEnter...")
        exit(1)
    
    # === 3. ZNAJD≈π NAJNOWSZY RUN ===
    print(f"\n‚è≥ Szukam najnowszego run GFS...")
    
    run_time, RUN_DATE, RUN_HOUR = find_latest_gfs_run(engine)
    
    if run_time is None:
        print(f"‚úó Nie znaleziono nowych danych GFS do pobrania")
        print(f"  (Wszystkie dostƒôpne runy sƒÖ ju≈º w bazie)")
        input("\nNaci≈õnij Enter...")
        exit(0)
    
    print(f"‚úì Znaleziono run do pobrania: {run_time.strftime('%Y-%m-%d %H:00')} UTC")
    print(f"  Data: {RUN_DATE}")
    print(f"  Cykl: {RUN_HOUR}Z")
    
    # === 4. SPRAWD≈π CO TRZEBA POBRAƒÜ ===
    print(f"\n‚è≥ Sprawdzam kt√≥re prognozy sƒÖ ju≈º w bazie...")
    
    required_hours = get_required_forecast_hours()
    existing_hours = get_existing_forecast_hours(run_time, engine)
    missing_hours = sorted(list(required_hours - existing_hours))
    
    print(f"  Wymagane: {len(required_hours)} prognoz (f000-f384)")
    print(f"  W bazie: {len(existing_hours)} prognoz")
    print(f"  Do pobrania: {len(missing_hours)} prognoz")
    
    if len(missing_hours) == 0:
        print(f"\n‚úì Wszystkie 209 prognoz sƒÖ ju≈º w bazie!")
        input("\nNaci≈õnij Enter...")
        exit(0)
    
    # === 5. POBIERANIE Z MULTI-THREADING ===
    print(f"\n{'='*70}")
    print(f"üöÄ ROZPOCZYNAM POBIERANIE (FILTERED VERSION)")
    print(f"{'='*70}")
    
    # Utw√≥rz katalog tymczasowy
    temp_dir = "temp_grib_filtered"
    os.makedirs(temp_dir, exist_ok=True)
    
    # Statystyki
    total_success = 0
    total_failed = 0
    total_records = 0
    total_bytes_filtered = 0
    total_bytes_full_estimate = 0
    
    # Kolejka zada≈Ñ
    download_queue = queue.Queue()
    progress_queue = queue.Queue()
    
    # Priorytetyzacja (niskie prognozy najpierw)
    for forecast_hour in missing_hours:
        download_queue.put(forecast_hour)
    
    start_time = time.time()
    
    def worker_thread_filtered():
        """WƒÖtek worker dla filtered version"""
        while True:
            try:
                forecast_hour = download_queue.get(timeout=1)
                if forecast_hour is None:
                    break
                
                # Buduj URL dla GRIB Filter
                url = build_grib_filter_url(RUN_DATE, RUN_HOUR, forecast_hour)
                
                # ≈öcie≈ºka do pliku tymczasowego
                temp_file = os.path.join(temp_dir, f"gfs_f{forecast_hour:03d}_filtered.grb2")
                
                # Pobierz plik (FILTERED!)
                success, file_size = download_grib_filtered(url, temp_file)
                
                if success:
                    # Przetw√≥rz i zapisz do bazy
                    try:
                        num_records = process_grib_to_db_filtered(
                            temp_file, run_time, forecast_hour,
                            lat_min, lat_max, lon_min, lon_max, engine
                        )
                        
                        # Szacuj rozmiar pe≈Çnego pliku (dla statystyk)
                        estimated_full_size = file_size * 10  # Oko≈Ço 10x wiƒôkszy
                        
                        # Wy≈õlij wynik
                        progress_queue.put({
                            'success': True,
                            'forecast_hour': forecast_hour,
                            'records': num_records,
                            'bytes_filtered': file_size,
                            'bytes_full_estimate': estimated_full_size
                        })
                        
                        # Usu≈Ñ plik tymczasowy
                        try:
                            os.remove(temp_file)
                        except:
                            pass
                            
                    except Exception as e:
                        module_logger.error(f"B≈ÇƒÖd przetwarzania f{forecast_hour:03d}: {e}")
                        progress_queue.put({
                            'success': False,
                            'forecast_hour': forecast_hour,
                            'records': 0,
                            'bytes_filtered': 0,
                            'bytes_full_estimate': 0
                        })
                else:
                    progress_queue.put({
                        'success': False,
                        'forecast_hour': forecast_hour,
                        'records': 0,
                        'bytes_filtered': 0,
                        'bytes_full_estimate': 0
                    })
                
                download_queue.task_done()
                
            except Empty:
                break
            except Exception as e:
                module_logger.error(f"B≈ÇƒÖd w worker thread: {e}")
                break
    
    # Uruchom wƒÖtki
    threads = []
    for i in range(NUM_THREADS):
        t = threading.Thread(target=worker_thread_filtered, daemon=True)
        t.start()
        threads.append(t)
    
    # Progress bar
    with tqdm(total=len(missing_hours), desc="Pobieranie", unit="prognoza") as pbar:
        completed = 0
        
        while completed < len(missing_hours):
            try:
                progress = progress_queue.get(timeout=1)
                completed += 1
                
                if progress['success']:
                    total_success += 1
                    total_records += progress['records']
                    total_bytes_filtered += progress['bytes_filtered']
                    total_bytes_full_estimate += progress['bytes_full_estimate']
                else:
                    total_failed += 1
                
                pbar.set_postfix({
                    'f': f"{progress['forecast_hour']:03d}",
                    'OK': total_success,
                    'FAIL': total_failed
                })
                pbar.update(1)
                
            except Empty:
                alive = sum(1 for t in threads if t.is_alive())
                if alive == 0:
                    break
    
    # Zako≈Ñcz wƒÖtki
    for _ in range(NUM_THREADS):
        download_queue.put(None)
    for t in threads:
        t.join(timeout=5)
    
    # === 6. PODSUMOWANIE ===
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    mb_filtered = total_bytes_filtered / (1024 * 1024)
    mb_full_estimate = total_bytes_full_estimate / (1024 * 1024)
    mb_saved = mb_full_estimate - mb_filtered
    percent_saved = (mb_saved / mb_full_estimate * 100) if mb_full_estimate > 0 else 0
    
    print("\n" + "=" * 70)
    print("‚úì‚úì‚úì POBRANIE ZAKO≈ÉCZONE!")
    print("=" * 70)
    print(f"Run GFS:           {run_time.strftime('%Y-%m-%d %H:00')} UTC")
    print(f"Prognoz pobrano:   {total_success}")
    print(f"Prognoz b≈Çƒôd√≥w:    {total_failed}")
    print(f"Rekord√≥w w bazie:  {total_records}")
    print(f"‚è±Ô∏è  Czas:             {elapsed_time:.1f}s")
    print(f"\nüìä STATYSTYKI FILTROWANIA:")
    print(f"  Pobrano (filtered):      {mb_filtered:.1f} MB")
    print(f"  Pe≈Çne pliki (szacunek):  {mb_full_estimate:.1f} MB")
    print(f"  üíæ OSZCZƒòDNO≈öƒÜ:          {mb_saved:.1f} MB ({percent_saved:.1f}%)")
    print("=" * 70)
    
    print(f"\nüí° Wszystkie dane sƒÖ ju≈º zapisane w bazie!")
    print(f"   Tabela: gfs_forecast")
    print(f"   Baza: {MYSQL_DATABASE}")
    
    input("\nNaci≈õnij Enter...")
